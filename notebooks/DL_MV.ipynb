{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba274863",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 10:47:16.494018: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-11 10:47:16.507755: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-06-11 10:47:16.605358: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-06-11 10:47:16.733378: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-06-11 10:47:16.840836: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-06-11 10:47:16.842211: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-06-11 10:47:16.998712: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI AVX_VNNI_INT8 AVX_NE_CONVERT FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-06-11 10:47:18.375593: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a266057",
   "metadata": {},
   "outputs": [],
   "source": [
    "path= \"../raw_data/train_df_ml_clean.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee9c544c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5be3665",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "360000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df = clean_df.sample(frac=0.1)\n",
    "len(clean_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7662e29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1330262</th>\n",
       "      <td>1</td>\n",
       "      <td>REALLY FUNNY: This was a great series finale, ...</td>\n",
       "      <td>really funny this wa a great series finale and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507063</th>\n",
       "      <td>1</td>\n",
       "      <td>another great sparks tear-jerker: this book is...</td>\n",
       "      <td>another great spark tearjerker this book is we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902322</th>\n",
       "      <td>0</td>\n",
       "      <td>Hard to get into: I started this book twice no...</td>\n",
       "      <td>hard to get into i started this book twice now...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3221197</th>\n",
       "      <td>0</td>\n",
       "      <td>Disappointed: Product was returned. Item made ...</td>\n",
       "      <td>disappointed product wa returned item made in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1782248</th>\n",
       "      <td>0</td>\n",
       "      <td>Nifty little tool but the door will break: Its...</td>\n",
       "      <td>nifty little tool but the door will break it a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         label                                               text  \\\n",
       "1330262      1  REALLY FUNNY: This was a great series finale, ...   \n",
       "507063       1  another great sparks tear-jerker: this book is...   \n",
       "902322       0  Hard to get into: I started this book twice no...   \n",
       "3221197      0  Disappointed: Product was returned. Item made ...   \n",
       "1782248      0  Nifty little tool but the door will break: Its...   \n",
       "\n",
       "                                                clean_text  \n",
       "1330262  really funny this wa a great series finale and...  \n",
       "507063   another great spark tearjerker this book is we...  \n",
       "902322   hard to get into i started this book twice now...  \n",
       "3221197  disappointed product wa returned item made in ...  \n",
       "1782248  nifty little tool but the door will break it a...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3194c0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = clean_df['clean_text'].to_numpy()\n",
    "y = clean_df['label'].to_numpy()\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8514ff4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['these are great a whole food vitamin and they taste yummy according to the kiddos they are easy to remember a they are only per day too',\n",
       "       'plaindisappointing this is a typical best seller that i fell into where the character are simple predictable in their illogical behavior sappy and boring the plot is of little relevence to any real life situation and the book wa neither educative nor entertaining i have a hard time knowing why there ha been such a favorable response to this work other than it doesnt tax one thought too much it isnt even on a par with oprahs book the cover is welldesigned and promising which go to confirm that you cant judge a book on it cover',\n",
       "       'dont sleep on kelly first of all to anyone that think that kelly cant sing do me a favorgo and cut off your ear and donate them to science because you obviously have no use for them at all i am really late on this one because i just picked up the album and i am not at all disappointed while i do feel that the production on a couple of song fall short simply deep doe a good job of maintaining a level of consistency throughout the project i really can appreciate an artist who is willing to take risk and experiment with different musical style kelly doe a good job with the material she wa given on this album i do however wish that there were more song worthy of being single that may have helped the album sale a little more i like most of the material but my favorite are lovehate and everytime you walk out that door the groove of this album is definitely smooth and i look forward to hearing more from kelly in the future',\n",
       "       'stupiduseless design these are virtually useless it is impossible to tighten the collar down tightly enough so they actually work the design is basically flawed stick to the vermont american style if you wish to get any work done a with these all you will be working on is these tool they simply are not worth the aggravation trying these wa a huge mistake i suspect i will have to take what i have left and epoxy the collar to the shaft just to get them to work at all noi am sorrybut they do not even work on soft pine these are just plain junk',\n",
       "       'fails to live up to it potential while the idea of this album paying tribute to the influence of stephen foster on american musical history is great the execution is ordinary at best the quality of the recording is also not very good it sound a if it were recorded at the bottom of a well',\n",
       "       'very informative for the new maltese owner i have found this book to be very informative for the new maltese owner i would recommend this book again',\n",
       "       'last a long time i am on my original detail srpray i use it inside and out glass and paint a very good product',\n",
       "       'bak derkderkallah derka derka mohammed jihad haka sherpasherpa abakala this book is much too violent and very cruel a an infidel i wa personally horrified by it passage however it give a good understanding for the conflict in the middle east',\n",
       "       'great recordcheap packaging for a 30th anniversary limited edition i expected a lot more the newly remastered sound is nothing spectacular sound way too sweet compared to the original lp soundthe packaging is just awful the sleeve is white cardboard not the glossy coated stock of the original the photo have the quality of a decadesold xerox rather than the crispness of photograph and the poster reproduction is so small and poorly printed you coudnt read the lyric even with a magnifying glassif it wasnt so much trouble i would probably ask emi for my money back',\n",
       "       'i would give it a big fat zero if i could this movie is the worst movie i have ever seen it dose not represent what truly happend on the ship at all iam a historian so i know dont waste your hard earn dollar on this rubbish'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9514611c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5d4bab2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[651,\n",
       " 184,\n",
       " 271,\n",
       " 207,\n",
       " 321,\n",
       " 435,\n",
       " 294,\n",
       " 653,\n",
       " 758,\n",
       " 569,\n",
       " 171,\n",
       " 245,\n",
       " 820,\n",
       " 387,\n",
       " 221,\n",
       " 457,\n",
       " 292,\n",
       " 142,\n",
       " 181,\n",
       " 129,\n",
       " 203,\n",
       " 122,\n",
       " 172,\n",
       " 647,\n",
       " 437,\n",
       " 654,\n",
       " 186,\n",
       " 108,\n",
       " 262,\n",
       " 144,\n",
       " 182,\n",
       " 143,\n",
       " 365,\n",
       " 268,\n",
       " 892,\n",
       " 607,\n",
       " 773,\n",
       " 123,\n",
       " 675,\n",
       " 126,\n",
       " 257,\n",
       " 301,\n",
       " 865,\n",
       " 331,\n",
       " 175,\n",
       " 950,\n",
       " 620,\n",
       " 149,\n",
       " 248,\n",
       " 219,\n",
       " 246,\n",
       " 702,\n",
       " 355,\n",
       " 928,\n",
       " 429,\n",
       " 109,\n",
       " 732,\n",
       " 640,\n",
       " 137,\n",
       " 375,\n",
       " 898,\n",
       " 258,\n",
       " 558,\n",
       " 145,\n",
       " 232,\n",
       " 343,\n",
       " 262,\n",
       " 293,\n",
       " 242,\n",
       " 403,\n",
       " 256,\n",
       " 312,\n",
       " 379,\n",
       " 598,\n",
       " 456,\n",
       " 456,\n",
       " 283,\n",
       " 469,\n",
       " 637,\n",
       " 491,\n",
       " 326,\n",
       " 220,\n",
       " 113,\n",
       " 809,\n",
       " 546,\n",
       " 296,\n",
       " 285,\n",
       " 635,\n",
       " 337,\n",
       " 450,\n",
       " 352,\n",
       " 457,\n",
       " 344,\n",
       " 119,\n",
       " 656,\n",
       " 566,\n",
       " 433,\n",
       " 150,\n",
       " 175,\n",
       " 793,\n",
       " 120,\n",
       " 119,\n",
       " 411,\n",
       " 873,\n",
       " 118,\n",
       " 274,\n",
       " 522,\n",
       " 637,\n",
       " 312,\n",
       " 785,\n",
       " 712,\n",
       " 962,\n",
       " 187,\n",
       " 520,\n",
       " 340,\n",
       " 535,\n",
       " 132,\n",
       " 153,\n",
       " 143,\n",
       " 628,\n",
       " 737,\n",
       " 180,\n",
       " 808,\n",
       " 342,\n",
       " 747,\n",
       " 187,\n",
       " 361,\n",
       " 355,\n",
       " 421,\n",
       " 164,\n",
       " 346,\n",
       " 301,\n",
       " 676,\n",
       " 383,\n",
       " 511,\n",
       " 443,\n",
       " 498,\n",
       " 637,\n",
       " 419,\n",
       " 254,\n",
       " 179,\n",
       " 219,\n",
       " 351,\n",
       " 242,\n",
       " 192,\n",
       " 769,\n",
       " 418,\n",
       " 431,\n",
       " 563,\n",
       " 408,\n",
       " 112,\n",
       " 91,\n",
       " 127,\n",
       " 667,\n",
       " 262,\n",
       " 392,\n",
       " 548,\n",
       " 421,\n",
       " 694,\n",
       " 832,\n",
       " 549,\n",
       " 128,\n",
       " 277,\n",
       " 678,\n",
       " 106,\n",
       " 231,\n",
       " 117,\n",
       " 440,\n",
       " 500,\n",
       " 101,\n",
       " 144,\n",
       " 101,\n",
       " 514,\n",
       " 592,\n",
       " 177,\n",
       " 354,\n",
       " 285,\n",
       " 911,\n",
       " 662,\n",
       " 324,\n",
       " 768,\n",
       " 794,\n",
       " 409,\n",
       " 678,\n",
       " 189,\n",
       " 196,\n",
       " 130,\n",
       " 654,\n",
       " 888,\n",
       " 148,\n",
       " 568,\n",
       " 363,\n",
       " 149,\n",
       " 349,\n",
       " 182,\n",
       " 254,\n",
       " 427,\n",
       " 514,\n",
       " 393,\n",
       " 297,\n",
       " 676,\n",
       " 577,\n",
       " 302,\n",
       " 275,\n",
       " 476,\n",
       " 600,\n",
       " 279,\n",
       " 201,\n",
       " 312,\n",
       " 832,\n",
       " 197,\n",
       " 537,\n",
       " 820,\n",
       " 205,\n",
       " 315,\n",
       " 234,\n",
       " 663,\n",
       " 401,\n",
       " 595,\n",
       " 504,\n",
       " 340,\n",
       " 290,\n",
       " 227,\n",
       " 238,\n",
       " 414,\n",
       " 471,\n",
       " 935,\n",
       " 220,\n",
       " 170,\n",
       " 333,\n",
       " 781,\n",
       " 507,\n",
       " 191,\n",
       " 375,\n",
       " 764,\n",
       " 389,\n",
       " 191,\n",
       " 199,\n",
       " 400,\n",
       " 315,\n",
       " 241,\n",
       " 225,\n",
       " 130,\n",
       " 181,\n",
       " 538,\n",
       " 157,\n",
       " 580,\n",
       " 499,\n",
       " 224,\n",
       " 271,\n",
       " 445,\n",
       " 650,\n",
       " 323,\n",
       " 395,\n",
       " 207,\n",
       " 490,\n",
       " 589,\n",
       " 246,\n",
       " 225,\n",
       " 287,\n",
       " 507,\n",
       " 172,\n",
       " 116,\n",
       " 471,\n",
       " 320,\n",
       " 952,\n",
       " 168,\n",
       " 893,\n",
       " 443,\n",
       " 530,\n",
       " 524,\n",
       " 328,\n",
       " 648,\n",
       " 573,\n",
       " 158,\n",
       " 605,\n",
       " 908,\n",
       " 761,\n",
       " 234,\n",
       " 384,\n",
       " 520,\n",
       " 465,\n",
       " 499,\n",
       " 247,\n",
       " 396,\n",
       " 213,\n",
       " 855,\n",
       " 947,\n",
       " 267,\n",
       " 768,\n",
       " 952,\n",
       " 161,\n",
       " 361,\n",
       " 222,\n",
       " 177,\n",
       " 146,\n",
       " 501,\n",
       " 791,\n",
       " 627,\n",
       " 130,\n",
       " 686,\n",
       " 455,\n",
       " 507,\n",
       " 202,\n",
       " 179,\n",
       " 284,\n",
       " 212,\n",
       " 170,\n",
       " 810,\n",
       " 678,\n",
       " 169,\n",
       " 534,\n",
       " 210,\n",
       " 165,\n",
       " 455,\n",
       " 195,\n",
       " 242,\n",
       " 155,\n",
       " 145,\n",
       " 136,\n",
       " 222,\n",
       " 181,\n",
       " 170,\n",
       " 274,\n",
       " 176,\n",
       " 398,\n",
       " 251,\n",
       " 129,\n",
       " 369,\n",
       " 138,\n",
       " 543,\n",
       " 461,\n",
       " 398,\n",
       " 234,\n",
       " 326,\n",
       " 589,\n",
       " 521,\n",
       " 286,\n",
       " 94,\n",
       " 576,\n",
       " 868,\n",
       " 360,\n",
       " 606,\n",
       " 367,\n",
       " 541,\n",
       " 449,\n",
       " 691,\n",
       " 123,\n",
       " 204,\n",
       " 166,\n",
       " 484,\n",
       " 577,\n",
       " 175,\n",
       " 366,\n",
       " 176,\n",
       " 151,\n",
       " 435,\n",
       " 470,\n",
       " 612,\n",
       " 740,\n",
       " 855,\n",
       " 162,\n",
       " 739,\n",
       " 115,\n",
       " 230,\n",
       " 405,\n",
       " 852,\n",
       " 261,\n",
       " 209,\n",
       " 224,\n",
       " 388,\n",
       " 669,\n",
       " 183,\n",
       " 173,\n",
       " 232,\n",
       " 356,\n",
       " 196,\n",
       " 850,\n",
       " 751,\n",
       " 313,\n",
       " 287,\n",
       " 340,\n",
       " 133,\n",
       " 458,\n",
       " 315,\n",
       " 745,\n",
       " 371,\n",
       " 139,\n",
       " 154,\n",
       " 771,\n",
       " 417,\n",
       " 245,\n",
       " 185,\n",
       " 445,\n",
       " 732,\n",
       " 330,\n",
       " 813,\n",
       " 636,\n",
       " 290,\n",
       " 825,\n",
       " 268,\n",
       " 530,\n",
       " 154,\n",
       " 122,\n",
       " 263,\n",
       " 930,\n",
       " 664,\n",
       " 516,\n",
       " 790,\n",
       " 168,\n",
       " 219,\n",
       " 882,\n",
       " 398,\n",
       " 346,\n",
       " 465,\n",
       " 313,\n",
       " 371,\n",
       " 472,\n",
       " 680,\n",
       " 302,\n",
       " 580,\n",
       " 260,\n",
       " 272,\n",
       " 501,\n",
       " 606,\n",
       " 251,\n",
       " 500,\n",
       " 327,\n",
       " 540,\n",
       " 170,\n",
       " 434,\n",
       " 116,\n",
       " 159,\n",
       " 128,\n",
       " 403,\n",
       " 605,\n",
       " 99,\n",
       " 300,\n",
       " 284,\n",
       " 229,\n",
       " 380,\n",
       " 742,\n",
       " 119,\n",
       " 345,\n",
       " 122,\n",
       " 121,\n",
       " 280,\n",
       " 130,\n",
       " 555,\n",
       " 150,\n",
       " 328,\n",
       " 479,\n",
       " 664,\n",
       " 803,\n",
       " 415,\n",
       " 379,\n",
       " 146,\n",
       " 281,\n",
       " 431,\n",
       " 380,\n",
       " 906,\n",
       " 290,\n",
       " 500,\n",
       " 502,\n",
       " 373,\n",
       " 255,\n",
       " 671,\n",
       " 540,\n",
       " 192,\n",
       " 859,\n",
       " 626,\n",
       " 535,\n",
       " 619,\n",
       " 757,\n",
       " 306,\n",
       " 433,\n",
       " 662,\n",
       " 460,\n",
       " 128,\n",
       " 460,\n",
       " 354,\n",
       " 408,\n",
       " 732,\n",
       " 646,\n",
       " 254,\n",
       " 624,\n",
       " 141,\n",
       " 274,\n",
       " 595,\n",
       " 440,\n",
       " 115,\n",
       " 576,\n",
       " 187,\n",
       " 822,\n",
       " 503,\n",
       " 272,\n",
       " 133,\n",
       " 739,\n",
       " 464,\n",
       " 556,\n",
       " 950,\n",
       " 173,\n",
       " 891,\n",
       " 328,\n",
       " 562,\n",
       " 551,\n",
       " 180,\n",
       " 526,\n",
       " 428,\n",
       " 281,\n",
       " 379,\n",
       " 186,\n",
       " 724,\n",
       " 515,\n",
       " 340,\n",
       " 138,\n",
       " 171,\n",
       " 357,\n",
       " 468,\n",
       " 712,\n",
       " 333,\n",
       " 186,\n",
       " 137,\n",
       " 202,\n",
       " 153,\n",
       " 650,\n",
       " 696,\n",
       " 332,\n",
       " 155,\n",
       " 181,\n",
       " 180,\n",
       " 112,\n",
       " 477,\n",
       " 290,\n",
       " 521,\n",
       " 446,\n",
       " 235,\n",
       " 903,\n",
       " 864,\n",
       " 669,\n",
       " 637,\n",
       " 222,\n",
       " 183,\n",
       " 832,\n",
       " 336,\n",
       " 257,\n",
       " 800,\n",
       " 426,\n",
       " 243,\n",
       " 767,\n",
       " 188,\n",
       " 287,\n",
       " 148,\n",
       " 404,\n",
       " 432,\n",
       " 125,\n",
       " 293,\n",
       " 382,\n",
       " 502,\n",
       " 297,\n",
       " 483,\n",
       " 423,\n",
       " 555,\n",
       " 886,\n",
       " 206,\n",
       " 855,\n",
       " 203,\n",
       " 403,\n",
       " 217,\n",
       " 519,\n",
       " 288,\n",
       " 411,\n",
       " 350,\n",
       " 198,\n",
       " 693,\n",
       " 442,\n",
       " 524,\n",
       " 130,\n",
       " 119,\n",
       " 109,\n",
       " 718,\n",
       " 337,\n",
       " 436,\n",
       " 204,\n",
       " 590,\n",
       " 359,\n",
       " 304,\n",
       " 416,\n",
       " 183,\n",
       " 108,\n",
       " 237,\n",
       " 641,\n",
       " 672,\n",
       " 320,\n",
       " 800,\n",
       " 250,\n",
       " 154,\n",
       " 145,\n",
       " 411,\n",
       " 137,\n",
       " 626,\n",
       " 338,\n",
       " 798,\n",
       " 242,\n",
       " 283,\n",
       " 470,\n",
       " 429,\n",
       " 464,\n",
       " 226,\n",
       " 436,\n",
       " 427,\n",
       " 630,\n",
       " 605,\n",
       " 411,\n",
       " 93,\n",
       " 821,\n",
       " 895,\n",
       " 739,\n",
       " 224,\n",
       " 609,\n",
       " 629,\n",
       " 639,\n",
       " 778,\n",
       " 264,\n",
       " 258,\n",
       " 273,\n",
       " 575,\n",
       " 480,\n",
       " 414,\n",
       " 451,\n",
       " 295,\n",
       " 402,\n",
       " 165,\n",
       " 295,\n",
       " 610,\n",
       " 289,\n",
       " 263,\n",
       " 457,\n",
       " 939,\n",
       " 211,\n",
       " 607,\n",
       " 657,\n",
       " 607,\n",
       " 203,\n",
       " 502,\n",
       " 219,\n",
       " 547,\n",
       " 351,\n",
       " 135,\n",
       " 514,\n",
       " 407,\n",
       " 646,\n",
       " 272,\n",
       " 446,\n",
       " 622,\n",
       " 160,\n",
       " 778,\n",
       " 161,\n",
       " 158,\n",
       " 548,\n",
       " 255,\n",
       " 332,\n",
       " 619,\n",
       " 232,\n",
       " 359,\n",
       " 127,\n",
       " 462,\n",
       " 247,\n",
       " 841,\n",
       " 445,\n",
       " 469,\n",
       " 967,\n",
       " 138,\n",
       " 463,\n",
       " 618,\n",
       " 475,\n",
       " 534,\n",
       " 306,\n",
       " 620,\n",
       " 303,\n",
       " 502,\n",
       " 304,\n",
       " 261,\n",
       " 149,\n",
       " 379,\n",
       " 358,\n",
       " 578,\n",
       " 186,\n",
       " 114,\n",
       " 527,\n",
       " 696,\n",
       " 252,\n",
       " 494,\n",
       " 170,\n",
       " 776,\n",
       " 699,\n",
       " 715,\n",
       " 420,\n",
       " 275,\n",
       " 384,\n",
       " 531,\n",
       " 344,\n",
       " 179,\n",
       " 457,\n",
       " 393,\n",
       " 970,\n",
       " 138,\n",
       " 201,\n",
       " 617,\n",
       " 93,\n",
       " 292,\n",
       " 677,\n",
       " 458,\n",
       " 568,\n",
       " 775,\n",
       " 135,\n",
       " 536,\n",
       " 215,\n",
       " 137,\n",
       " 203,\n",
       " 364,\n",
       " 279,\n",
       " 344,\n",
       " 216,\n",
       " 309,\n",
       " 216,\n",
       " 320,\n",
       " 850,\n",
       " 444,\n",
       " 541,\n",
       " 298,\n",
       " 177,\n",
       " 187,\n",
       " 489,\n",
       " 667,\n",
       " 625,\n",
       " 195,\n",
       " 792,\n",
       " 301,\n",
       " 300,\n",
       " 130,\n",
       " 469,\n",
       " 308,\n",
       " 370,\n",
       " 342,\n",
       " 516,\n",
       " 428,\n",
       " 514,\n",
       " 559,\n",
       " 333,\n",
       " 956,\n",
       " 774,\n",
       " 157,\n",
       " 326,\n",
       " 280,\n",
       " 148,\n",
       " 472,\n",
       " 958,\n",
       " 606,\n",
       " 259,\n",
       " 162,\n",
       " 200,\n",
       " 163,\n",
       " 708,\n",
       " 232,\n",
       " 734,\n",
       " 426,\n",
       " 472,\n",
       " 151,\n",
       " 578,\n",
       " 432,\n",
       " 663,\n",
       " 435,\n",
       " 305,\n",
       " 709,\n",
       " 210,\n",
       " 208,\n",
       " 284,\n",
       " 279,\n",
       " 737,\n",
       " 406,\n",
       " 234,\n",
       " 228,\n",
       " 383,\n",
       " 515,\n",
       " 284,\n",
       " 154,\n",
       " 383,\n",
       " 159,\n",
       " 571,\n",
       " 694,\n",
       " 973,\n",
       " 442,\n",
       " 156,\n",
       " 149,\n",
       " 699,\n",
       " 300,\n",
       " 579,\n",
       " 416,\n",
       " 595,\n",
       " 511,\n",
       " 583,\n",
       " 423,\n",
       " 273,\n",
       " 762,\n",
       " 617,\n",
       " 781,\n",
       " 494,\n",
       " 125,\n",
       " 257,\n",
       " 235,\n",
       " 869,\n",
       " 220,\n",
       " 565,\n",
       " 418,\n",
       " 198,\n",
       " 212,\n",
       " 287,\n",
       " 103,\n",
       " 547,\n",
       " 700,\n",
       " 257,\n",
       " 221,\n",
       " 324,\n",
       " 267,\n",
       " 182,\n",
       " 317,\n",
       " 294,\n",
       " 334,\n",
       " 147,\n",
       " 158,\n",
       " 228,\n",
       " 296,\n",
       " 169,\n",
       " 247,\n",
       " 566,\n",
       " 459,\n",
       " 787,\n",
       " 427,\n",
       " 444,\n",
       " 433,\n",
       " 129,\n",
       " 339,\n",
       " 120,\n",
       " 202,\n",
       " 436,\n",
       " 596,\n",
       " 432,\n",
       " 579,\n",
       " 112,\n",
       " 178,\n",
       " 675,\n",
       " 555,\n",
       " 380,\n",
       " 342,\n",
       " 721,\n",
       " 288,\n",
       " 210,\n",
       " 346,\n",
       " 560,\n",
       " 282,\n",
       " 217,\n",
       " 234,\n",
       " 295,\n",
       " 433,\n",
       " 379,\n",
       " 282,\n",
       " 208,\n",
       " 379,\n",
       " 123,\n",
       " 394,\n",
       " 430,\n",
       " 476,\n",
       " 184,\n",
       " 850,\n",
       " 215,\n",
       " 623,\n",
       " 135,\n",
       " 512,\n",
       " 665,\n",
       " 340,\n",
       " 379,\n",
       " 312,\n",
       " 303,\n",
       " 722,\n",
       " 193,\n",
       " 632,\n",
       " 421,\n",
       " 334,\n",
       " 579,\n",
       " 850,\n",
       " 195,\n",
       " 118,\n",
       " 881,\n",
       " 278,\n",
       " 811,\n",
       " 336,\n",
       " 627,\n",
       " 111,\n",
       " 619,\n",
       " 766,\n",
       " 839,\n",
       " 272,\n",
       " 144,\n",
       " 288,\n",
       " 639,\n",
       " 684,\n",
       " 370,\n",
       " 122,\n",
       " 419,\n",
       " 218,\n",
       " 348,\n",
       " 358,\n",
       " 171,\n",
       " 544,\n",
       " 648,\n",
       " 819,\n",
       " 406,\n",
       " 244,\n",
       " 165,\n",
       " 964,\n",
       " 885,\n",
       " 197,\n",
       " 306,\n",
       " 442,\n",
       " 378,\n",
       " 440,\n",
       " 599,\n",
       " 355,\n",
       " 259,\n",
       " 497,\n",
       " 183,\n",
       " 532,\n",
       " 588,\n",
       " 610,\n",
       " 152,\n",
       " 129,\n",
       " 322,\n",
       " 531,\n",
       " 605,\n",
       " 360,\n",
       " 750,\n",
       " 115,\n",
       " 448,\n",
       " 206,\n",
       " 144,\n",
       " 173,\n",
       " 332,\n",
       " 743,\n",
       " 175,\n",
       " 364,\n",
       " 224,\n",
       " 92,\n",
       " 592,\n",
       " 190,\n",
       " 425,\n",
       " 464,\n",
       " 798,\n",
       " 135,\n",
       " 566,\n",
       " 549,\n",
       " 113,\n",
       " 391,\n",
       " 328,\n",
       " 897,\n",
       " 124,\n",
       " 252,\n",
       " 158,\n",
       " 932,\n",
       " 510,\n",
       " 104,\n",
       " 159,\n",
       " 245,\n",
       " 177,\n",
       " 493,\n",
       " 239,\n",
       " 352,\n",
       " 258,\n",
       " 704,\n",
       " 633,\n",
       " 657,\n",
       " 578,\n",
       " 935,\n",
       " 667,\n",
       " 152,\n",
       " 958,\n",
       " 398,\n",
       " 221,\n",
       " 966,\n",
       " 224,\n",
       " 264,\n",
       " 685,\n",
       " 637,\n",
       " 403,\n",
       " 723,\n",
       " 163,\n",
       " 751,\n",
       " 573,\n",
       " 379,\n",
       " 135,\n",
       " ...]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length_array = [len(seq) for seq in X_train.astype(str)]\n",
    "length_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a3986d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([20147., 57566., 49006., 40854., 33486., 27078., 21349., 16990.,\n",
       "        13491.,  8033.]),\n",
       " array([ 38. , 133.4, 228.8, 324.2, 419.6, 515. , 610.4, 705.8, 801.2,\n",
       "        896.6, 992. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjkAAAGgCAYAAABIanZ7AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjMsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvZiW1igAAAAlwSFlzAAAPYQAAD2EBqD+naQAALdNJREFUeJzt3X90VPWd//FXfjCT8GMSfk5AgqSLBSLIjwBh/LXLmmW0sSuKXWApTRFloYEKafnlj6BubVjcreLyS+up4ZyK/DinohAJmxME1jLyIxglaCJdcZMKk6CQDFBIIPP5/rHf3DIFLUEg5sPzcc49h7mf973zvh8g8zo3996JMsYYAQAAWCa6pRsAAAC4Ggg5AADASoQcAABgJUIOAACwEiEHAABYiZADAACsRMgBAABWIuQAAAArEXIAAICVCDkAAMBKzQ45n3/+uX74wx+qc+fOio+P18CBA7V3715n3Bij3Nxcde/eXfHx8crIyNDBgwcj9nHs2DFNnDhRHo9HiYmJmjJlik6ePBlR8+GHH+qOO+5QXFyckpOTtXjx4gt6Wb9+vfr166e4uDgNHDhQb7/9dnMPBwAAWCq2OcXHjx/XbbfdplGjRmnz5s3q2rWrDh48qI4dOzo1ixcv1osvvqhVq1YpJSVFTz75pPx+vz766CPFxcVJkiZOnKgjR46oqKhIZ8+e1eTJkzV16lStXr1akhQKhTR69GhlZGRo5cqV2r9/vx566CElJiZq6tSpkqSdO3dqwoQJysvL07333qvVq1drzJgx2rdvnwYMGHBJxxMOh3X48GF16NBBUVFRzZkKAADQQowxOnHihHr06KHo6K85X2OaYd68eeb222//yvFwOGySkpLMc88956yrra01brfbvP7668YYYz766CMjyezZs8ep2bx5s4mKijKff/65McaY5cuXm44dO5r6+vqI9+7bt6/z+p/+6Z9MZmZmxPunp6ebf/mXf7nk46mqqjKSWFhYWFhYWFrhUlVV9bWf8806k/PWW2/J7/frBz/4gbZv364bbrhBP/nJT/TII49Ikg4dOqRgMKiMjAxnm4SEBKWnpysQCGj8+PEKBAJKTEzUsGHDnJqMjAxFR0dr165duv/++xUIBHTnnXfK5XI5NX6/X//2b/+m48ePq2PHjgoEAsrJyYnoz+/3a8OGDV/Zf319verr653X5v9/AXtVVZU8Hk9zpgIAALSQUCik5ORkdejQ4WvrmhVyPv30U61YsUI5OTl67LHHtGfPHv30pz+Vy+VSVlaWgsGgJMnr9UZs5/V6nbFgMKhu3bpFNhEbq06dOkXUpKSkXLCPprGOHTsqGAx+7ftcTF5enp5++ukL1ns8HkIOAACtzF+71KRZFx6Hw2ENHTpUv/zlLzVkyBBNnTpVjzzyiFauXPmNmrxWFixYoLq6Omepqqpq6ZYAAMBV0qyQ0717d6Wmpkas69+/vyorKyVJSUlJkqTq6uqImurqamcsKSlJNTU1EePnzp3TsWPHImouto/z3+OraprGL8btdjtnbTh7AwCA3ZoVcm677TZVVFRErPvkk0904403SpJSUlKUlJSk4uJiZzwUCmnXrl3y+XySJJ/Pp9raWpWUlDg1W7duVTgcVnp6ulOzY8cOnT171qkpKipS3759nTu5fD5fxPs01TS9DwAAuM5d8q1Ixpjdu3eb2NhY8+yzz5qDBw+a1157zbRt29b89re/dWoWLVpkEhMTzZtvvmk+/PBDc99995mUlBRz+vRpp+buu+82Q4YMMbt27TLvvvuuuemmm8yECROc8draWuP1es2kSZNMWVmZWbNmjWnbtq156aWXnJrf//73JjY21vz7v/+7+fjjj83ChQtNmzZtzP79+y/5eOrq6owkU1dX15xpAAAALehSP7+bFXKMMWbjxo1mwIABxu12m379+pmXX345YjwcDpsnn3zSeL1e43a7zV133WUqKioiar788kszYcIE0759e+PxeMzkyZPNiRMnImo++OADc/vttxu3221uuOEGs2jRogt6Wbdunfnud79rXC6Xufnmm01BQUGzjoWQAwBA63Opn99Rxvz/+6ivQ6FQSAkJCaqrq+P6HAAAWolL/fzmu6sAAICVCDkAAMBKhBwAAGAlQg4AALASIQcAAFiJkAMAAKxEyAEAAFYi5AAAACvFtnQD+PboPb+gpVtots8WZbZ0CwCAbynO5AAAACsRcgAAgJUIOQAAwEqEHAAAYCVCDgAAsBIhBwAAWImQAwAArETIAQAAViLkAAAAKxFyAACAlQg5AADASoQcAABgJUIOAACwEiEHAABYiZADAACsRMgBAABWIuQAAAArEXIAAICVCDkAAMBKhBwAAGAlQg4AALASIQcAAFiJkAMAAKxEyAEAAFYi5AAAACsRcgAAgJUIOQAAwEqEHAAAYCVCDgAAsBIhBwAAWImQAwAArETIAQAAViLkAAAAKxFyAACAlQg5AADASoQcAABgJUIOAACwEiEHAABYiZADAACsRMgBAABWIuQAAAArNSvkPPXUU4qKiopY+vXr54yfOXNG2dnZ6ty5s9q3b6+xY8equro6Yh+VlZXKzMxU27Zt1a1bN82ZM0fnzp2LqNm2bZuGDh0qt9utPn36KD8//4Jeli1bpt69eysuLk7p6enavXt3cw4FAABYrtlncm6++WYdOXLEWd59911nbPbs2dq4caPWr1+v7du36/Dhw3rggQec8cbGRmVmZqqhoUE7d+7UqlWrlJ+fr9zcXKfm0KFDyszM1KhRo1RaWqpZs2bp4Ycf1pYtW5yatWvXKicnRwsXLtS+ffs0aNAg+f1+1dTUXO48AAAAy0QZY8ylFj/11FPasGGDSktLLxirq6tT165dtXr1aj344IOSpPLycvXv31+BQEAjR47U5s2bde+99+rw4cPyer2SpJUrV2revHk6evSoXC6X5s2bp4KCApWVlTn7Hj9+vGpra1VYWChJSk9P1/Dhw7V06VJJUjgcVnJysmbOnKn58+df8sGHQiElJCSorq5OHo/nkrezVe/5BS3dQrN9tiizpVsAAFxjl/r53ewzOQcPHlSPHj30ne98RxMnTlRlZaUkqaSkRGfPnlVGRoZT269fP/Xq1UuBQECSFAgENHDgQCfgSJLf71coFNKBAwecmvP30VTTtI+GhgaVlJRE1ERHRysjI8OpAQAAiG1OcXp6uvLz89W3b18dOXJETz/9tO644w6VlZUpGAzK5XIpMTExYhuv16tgMChJCgaDEQGnabxp7OtqQqGQTp8+rePHj6uxsfGiNeXl5V/bf319verr653XoVDo0g8eAAC0Ks0KOffcc4/z51tuuUXp6em68cYbtW7dOsXHx1/x5q60vLw8Pf300y3dBgAAuAa+0S3kiYmJ+u53v6s//OEPSkpKUkNDg2prayNqqqurlZSUJElKSkq64G6rptd/rcbj8Sg+Pl5dunRRTEzMRWua9vFVFixYoLq6Omepqqpq9jEDAIDW4RuFnJMnT+p//ud/1L17d6WlpalNmzYqLi52xisqKlRZWSmfzydJ8vl82r9/f8RdUEVFRfJ4PEpNTXVqzt9HU03TPlwul9LS0iJqwuGwiouLnZqv4na75fF4IhYAAGCnZoWcn//859q+fbs+++wz7dy5U/fff79iYmI0YcIEJSQkaMqUKcrJydE777yjkpISTZ48WT6fTyNHjpQkjR49WqmpqZo0aZI++OADbdmyRU888YSys7PldrslSdOmTdOnn36quXPnqry8XMuXL9e6des0e/Zsp4+cnBz9+te/1qpVq/Txxx9r+vTpOnXqlCZPnnwFpwYAALRmzbom549//KMmTJigL7/8Ul27dtXtt9+u9957T127dpUkPf/884qOjtbYsWNVX18vv9+v5cuXO9vHxMRo06ZNmj59unw+n9q1a6esrCw988wzTk1KSooKCgo0e/ZsLVmyRD179tQrr7wiv9/v1IwbN05Hjx5Vbm6ugsGgBg8erMLCwgsuRgYAANevZj0nxzY8JycSz8kBALQGl/r53awzOcC3DcEMAPBV+IJOAABgJUIOAACwEiEHAABYiZADAACsRMgBAABWIuQAAAArEXIAAICVCDkAAMBKhBwAAGAlQg4AALASIQcAAFiJkAMAAKxEyAEAAFYi5AAAACsRcgAAgJUIOQAAwEqEHAAAYCVCDgAAsBIhBwAAWImQAwAArETIAQAAViLkAAAAKxFyAACAlQg5AADASoQcAABgJUIOAACwEiEHAABYiZADAACsRMgBAABWIuQAAAArEXIAAICVCDkAAMBKhBwAAGAlQg4AALASIQcAAFiJkAMAAKxEyAEAAFYi5AAAACsRcgAAgJUIOQAAwEqEHAAAYCVCDgAAsBIhBwAAWImQAwAArETIAQAAViLkAAAAKxFyAACAlQg5AADASrEt3QBwvek9v6ClW7gsny3KbOkWAKBZvtGZnEWLFikqKkqzZs1y1p05c0bZ2dnq3Lmz2rdvr7Fjx6q6ujpiu8rKSmVmZqpt27bq1q2b5syZo3PnzkXUbNu2TUOHDpXb7VafPn2Un59/wfsvW7ZMvXv3VlxcnNLT07V79+5vcjgAAMAilx1y9uzZo5deekm33HJLxPrZs2dr48aNWr9+vbZv367Dhw/rgQcecMYbGxuVmZmphoYG7dy5U6tWrVJ+fr5yc3OdmkOHDikzM1OjRo1SaWmpZs2apYcfflhbtmxxatauXaucnBwtXLhQ+/bt06BBg+T3+1VTU3O5hwQAACwSZYwxzd3o5MmTGjp0qJYvX65f/OIXGjx4sF544QXV1dWpa9euWr16tR588EFJUnl5ufr3769AIKCRI0dq8+bNuvfee3X48GF5vV5J0sqVKzVv3jwdPXpULpdL8+bNU0FBgcrKypz3HD9+vGpra1VYWChJSk9P1/Dhw7V06VJJUjgcVnJysmbOnKn58+df0nGEQiElJCSorq5OHo+nudNgndb6axRcG/y6CsC3xaV+fl/WmZzs7GxlZmYqIyMjYn1JSYnOnj0bsb5fv37q1auXAoGAJCkQCGjgwIFOwJEkv9+vUCikAwcOODV/uW+/3+/so6GhQSUlJRE10dHRysjIcGoAAMD1rdkXHq9Zs0b79u3Tnj17LhgLBoNyuVxKTEyMWO/1ehUMBp2a8wNO03jT2NfVhEIhnT59WsePH1djY+NFa8rLy7+y9/r6etXX1zuvQ6HQXzlaAADQWjXrTE5VVZUeffRRvfbaa4qLi7taPV01eXl5SkhIcJbk5OSWbgkAAFwlzQo5JSUlqqmp0dChQxUbG6vY2Fht375dL774omJjY+X1etXQ0KDa2tqI7aqrq5WUlCRJSkpKuuBuq6bXf63G4/EoPj5eXbp0UUxMzEVrmvZxMQsWLFBdXZ2zVFVVNefwAQBAK9KskHPXXXdp//79Ki0tdZZhw4Zp4sSJzp/btGmj4uJiZ5uKigpVVlbK5/NJknw+n/bv3x9xF1RRUZE8Ho9SU1OdmvP30VTTtA+Xy6W0tLSImnA4rOLiYqfmYtxutzweT8QCAADs1Kxrcjp06KABAwZErGvXrp06d+7srJ8yZYpycnLUqVMneTwezZw5Uz6fTyNHjpQkjR49WqmpqZo0aZIWL16sYDCoJ554QtnZ2XK73ZKkadOmaenSpZo7d64eeughbd26VevWrVNBwZ/v/snJyVFWVpaGDRumESNG6IUXXtCpU6c0efLkbzQhAADADlf8icfPP/+8oqOjNXbsWNXX18vv92v58uXOeExMjDZt2qTp06fL5/OpXbt2ysrK0jPPPOPUpKSkqKCgQLNnz9aSJUvUs2dPvfLKK/L7/U7NuHHjdPToUeXm5ioYDGrw4MEqLCy84GJkAABwfbqs5+TYgufkROI5Ofg6PCcHwLfFVX1ODgAAwLcdIQcAAFiJkAMAAKxEyAEAAFYi5AAAACsRcgAAgJUIOQAAwEqEHAAAYCVCDgAAsBIhBwAAWImQAwAArETIAQAAViLkAAAAKxFyAACAlQg5AADASoQcAABgJUIOAACwEiEHAABYiZADAACsRMgBAABWIuQAAAArEXIAAICVCDkAAMBKhBwAAGAlQg4AALASIQcAAFiJkAMAAKxEyAEAAFYi5AAAACsRcgAAgJUIOQAAwEqEHAAAYCVCDgAAsBIhBwAAWCm2pRsA0Dr0nl/Q0i0022eLMlu6BQAtiDM5AADASoQcAABgJUIOAACwEiEHAABYiZADAACsRMgBAABWIuQAAAArEXIAAICVCDkAAMBKhBwAAGAlQg4AALASIQcAAFiJkAMAAKxEyAEAAFYi5AAAACs1K+SsWLFCt9xyizwejzwej3w+nzZv3uyMnzlzRtnZ2ercubPat2+vsWPHqrq6OmIflZWVyszMVNu2bdWtWzfNmTNH586di6jZtm2bhg4dKrfbrT59+ig/P/+CXpYtW6bevXsrLi5O6enp2r17d3MOBQAAWK5ZIadnz55atGiRSkpKtHfvXv393/+97rvvPh04cECSNHv2bG3cuFHr16/X9u3bdfjwYT3wwAPO9o2NjcrMzFRDQ4N27typVatWKT8/X7m5uU7NoUOHlJmZqVGjRqm0tFSzZs3Sww8/rC1btjg1a9euVU5OjhYuXKh9+/Zp0KBB8vv9qqmp+abzAQAALBFljDHfZAedOnXSc889pwcffFBdu3bV6tWr9eCDD0qSysvL1b9/fwUCAY0cOVKbN2/Wvffeq8OHD8vr9UqSVq5cqXnz5uno0aNyuVyaN2+eCgoKVFZW5rzH+PHjVVtbq8LCQklSenq6hg8frqVLl0qSwuGwkpOTNXPmTM2fP/+Sew+FQkpISFBdXZ08Hs83mQYr9J5f0NItAFfUZ4syW7oFAFfBpX5+X/Y1OY2NjVqzZo1OnToln8+nkpISnT17VhkZGU5Nv3791KtXLwUCAUlSIBDQwIEDnYAjSX6/X6FQyDkbFAgEIvbRVNO0j4aGBpWUlETUREdHKyMjw6kBAACIbe4G+/fvl8/n05kzZ9S+fXu98cYbSk1NVWlpqVwulxITEyPqvV6vgsGgJCkYDEYEnKbxprGvqwmFQjp9+rSOHz+uxsbGi9aUl5d/be/19fWqr693XodCoUs/cAAA0Ko0+0xO3759VVpaql27dmn69OnKysrSRx99dDV6u+Ly8vKUkJDgLMnJyS3dEgAAuEqaHXJcLpf69OmjtLQ05eXladCgQVqyZImSkpLU0NCg2traiPrq6molJSVJkpKSki6426rp9V+r8Xg8io+PV5cuXRQTE3PRmqZ9fJUFCxaorq7OWaqqqpp7+AAAoJX4xs/JCYfDqq+vV1pamtq0aaPi4mJnrKKiQpWVlfL5fJIkn8+n/fv3R9wFVVRUJI/Ho9TUVKfm/H001TTtw+VyKS0tLaImHA6ruLjYqfkqbrfbuf29aQEAAHZq1jU5CxYs0D333KNevXrpxIkTWr16tbZt26YtW7YoISFBU6ZMUU5Ojjp16iSPx6OZM2fK5/Np5MiRkqTRo0crNTVVkyZN0uLFixUMBvXEE08oOztbbrdbkjRt2jQtXbpUc+fO1UMPPaStW7dq3bp1Kij4850/OTk5ysrK0rBhwzRixAi98MILOnXqlCZPnnwFpwYAALRmzQo5NTU1+tGPfqQjR44oISFBt9xyi7Zs2aJ/+Id/kCQ9//zzio6O1tixY1VfXy+/36/ly5c728fExGjTpk2aPn26fD6f2rVrp6ysLD3zzDNOTUpKigoKCjR79mwtWbJEPXv21CuvvCK/3+/UjBs3TkePHlVubq6CwaAGDx6swsLCCy5GBgAA169v/Jyc1ozn5ETiOTmwDc/JAex01Z+TAwAA8G1GyAEAAFYi5AAAACsRcgAAgJUIOQAAwEqEHAAAYCVCDgAAsBIhBwAAWImQAwAArETIAQAAViLkAAAAKxFyAACAlZr1LeQA0Jq0xi+d5UtFgSuHMzkAAMBKhBwAAGAlQg4AALASIQcAAFiJkAMAAKxEyAEAAFYi5AAAACsRcgAAgJUIOQAAwEqEHAAAYCVCDgAAsBIhBwAAWImQAwAArETIAQAAViLkAAAAKxFyAACAlQg5AADASoQcAABgJUIOAACwEiEHAABYiZADAACsRMgBAABWIuQAAAArEXIAAICVCDkAAMBKhBwAAGAlQg4AALASIQcAAFiJkAMAAKxEyAEAAFYi5AAAACsRcgAAgJUIOQAAwEqEHAAAYKXYlm4AAPBnvecXtHQLzfbZosyWbgG4KM7kAAAAKxFyAACAlQg5AADASs0KOXl5eRo+fLg6dOigbt26acyYMaqoqIioOXPmjLKzs9W5c2e1b99eY8eOVXV1dURNZWWlMjMz1bZtW3Xr1k1z5szRuXPnImq2bdumoUOHyu12q0+fPsrPz7+gn2XLlql3796Ki4tTenq6du/e3ZzDAQAAFmtWyNm+fbuys7P13nvvqaioSGfPntXo0aN16tQpp2b27NnauHGj1q9fr+3bt+vw4cN64IEHnPHGxkZlZmaqoaFBO3fu1KpVq5Sfn6/c3Fyn5tChQ8rMzNSoUaNUWlqqWbNm6eGHH9aWLVucmrVr1yonJ0cLFy7Uvn37NGjQIPn9ftXU1HyT+QAAAJaIMsaYy9346NGj6tatm7Zv364777xTdXV16tq1q1avXq0HH3xQklReXq7+/fsrEAho5MiR2rx5s+69914dPnxYXq9XkrRy5UrNmzdPR48elcvl0rx581RQUKCysjLnvcaPH6/a2loVFhZKktLT0zV8+HAtXbpUkhQOh5WcnKyZM2dq/vz5l9R/KBRSQkKC6urq5PF4LncarNEa7+oA0PK4uwrX2qV+fn+jW8jr6uokSZ06dZIklZSU6OzZs8rIyHBq+vXrp169ejkhJxAIaODAgU7AkSS/36/p06frwIEDGjJkiAKBQMQ+mmpmzZolSWpoaFBJSYkWLFjgjEdHRysjI0OBQOAr+62vr1d9fb3zOhQKXf7B/xUEBgAAWtZlX3gcDoc1a9Ys3XbbbRowYIAkKRgMyuVyKTExMaLW6/UqGAw6NecHnKbxprGvqwmFQjp9+rS++OILNTY2XrSmaR8Xk5eXp4SEBGdJTk5u/oEDAIBW4bJDTnZ2tsrKyrRmzZor2c9VtWDBAtXV1TlLVVVVS7cEAACuksv6ddWMGTO0adMm7dixQz179nTWJyUlqaGhQbW1tRFnc6qrq5WUlOTU/OVdUE13X51f85d3ZFVXV8vj8Sg+Pl4xMTGKiYm5aE3TPi7G7XbL7XY3/4ABAECr06wzOcYYzZgxQ2+88Ya2bt2qlJSUiPG0tDS1adNGxcXFzrqKigpVVlbK5/NJknw+n/bv3x9xF1RRUZE8Ho9SU1OdmvP30VTTtA+Xy6W0tLSImnA4rOLiYqcGAABc35p1Jic7O1urV6/Wm2++qQ4dOjjXvyQkJCg+Pl4JCQmaMmWKcnJy1KlTJ3k8Hs2cOVM+n08jR46UJI0ePVqpqamaNGmSFi9erGAwqCeeeELZ2dnOWZZp06Zp6dKlmjt3rh566CFt3bpV69atU0HBny/mzcnJUVZWloYNG6YRI0bohRde0KlTpzR58uQrNTcAAKAVa1bIWbFihSTp7/7u7yLWv/rqq/rxj38sSXr++ecVHR2tsWPHqr6+Xn6/X8uXL3dqY2JitGnTJk2fPl0+n0/t2rVTVlaWnnnmGacmJSVFBQUFmj17tpYsWaKePXvqlVdekd/vd2rGjRuno0ePKjc3V8FgUIMHD1ZhYeEFFyMDAIDr0zd6Tk5rdzWfk8Mt5ACuFzwnB9fapX5+891VAADASoQcAABgJUIOAACwEiEHAABYiZADAACsRMgBAABWIuQAAAArEXIAAICVCDkAAMBKhBwAAGAlQg4AALBSs76gEwCAv9Qav6uP79u6PnAmBwAAWImQAwAArETIAQAAViLkAAAAKxFyAACAlQg5AADASoQcAABgJUIOAACwEiEHAABYiZADAACsRMgBAABWIuQAAAArEXIAAICVCDkAAMBKhBwAAGAlQg4AALASIQcAAFiJkAMAAKxEyAEAAFYi5AAAACsRcgAAgJViW7oBAACutd7zC1q6hWb7bFFmS7fQ6nAmBwAAWImQAwAArETIAQAAViLkAAAAKxFyAACAlQg5AADASoQcAABgJUIOAACwEiEHAABYiZADAACsRMgBAABWIuQAAAArEXIAAICVCDkAAMBKhBwAAGAlQg4AALBSs0POjh079P3vf189evRQVFSUNmzYEDFujFFubq66d++u+Ph4ZWRk6ODBgxE1x44d08SJE+XxeJSYmKgpU6bo5MmTETUffvih7rjjDsXFxSk5OVmLFy++oJf169erX79+iouL08CBA/X2228393AAAIClmh1yTp06pUGDBmnZsmUXHV+8eLFefPFFrVy5Urt27VK7du3k9/t15swZp2bixIk6cOCAioqKtGnTJu3YsUNTp051xkOhkEaPHq0bb7xRJSUleu655/TUU0/p5Zdfdmp27typCRMmaMqUKXr//fc1ZswYjRkzRmVlZc09JAAAYKEoY4y57I2jovTGG29ozJgxkv7vLE6PHj30s5/9TD//+c8lSXV1dfJ6vcrPz9f48eP18ccfKzU1VXv27NGwYcMkSYWFhfre976nP/7xj+rRo4dWrFihxx9/XMFgUC6XS5I0f/58bdiwQeXl5ZKkcePG6dSpU9q0aZPTz8iRIzV48GCtXLnykvoPhUJKSEhQXV2dPB7P5U7DRfWeX3BF9wcAuL59tiizpVv41rjUz+8rek3OoUOHFAwGlZGR4axLSEhQenq6AoGAJCkQCCgxMdEJOJKUkZGh6Oho7dq1y6m58847nYAjSX6/XxUVFTp+/LhTc/77NNU0vc/F1NfXKxQKRSwAAMBOVzTkBINBSZLX641Y7/V6nbFgMKhu3bpFjMfGxqpTp04RNRfbx/nv8VU1TeMXk5eXp4SEBGdJTk5u7iECAIBW4rq6u2rBggWqq6tzlqqqqpZuCQAAXCVXNOQkJSVJkqqrqyPWV1dXO2NJSUmqqamJGD937pyOHTsWUXOxfZz/Hl9V0zR+MW63Wx6PJ2IBAAB2uqIhJyUlRUlJSSouLnbWhUIh7dq1Sz6fT5Lk8/lUW1urkpISp2br1q0Kh8NKT093anbs2KGzZ886NUVFRerbt686duzo1Jz/Pk01Te8DAACub80OOSdPnlRpaalKS0sl/d/FxqWlpaqsrFRUVJRmzZqlX/ziF3rrrbe0f/9+/ehHP1KPHj2cO7D69++vu+++W4888oh2796t3//+95oxY4bGjx+vHj16SJL++Z//WS6XS1OmTNGBAwe0du1aLVmyRDk5OU4fjz76qAoLC/Uf//EfKi8v11NPPaW9e/dqxowZ33xWAABAqxfb3A327t2rUaNGOa+bgkdWVpby8/M1d+5cnTp1SlOnTlVtba1uv/12FRYWKi4uztnmtdde04wZM3TXXXcpOjpaY8eO1YsvvuiMJyQk6L/+67+UnZ2ttLQ0denSRbm5uRHP0rn11lu1evVqPfHEE3rsscd00003acOGDRowYMBlTQQAALDLN3pOTmvHc3IAAK0Fz8n5sxZ5Tg4AAMC3BSEHAABYiZADAACsRMgBAABWIuQAAAArNfsWcgAAcO21xrt2W/qOMM7kAAAAKxFyAACAlQg5AADASoQcAABgJUIOAACwEiEHAABYiZADAACsRMgBAABWIuQAAAArEXIAAICVCDkAAMBKhBwAAGAlQg4AALASIQcAAFiJkAMAAKxEyAEAAFYi5AAAACsRcgAAgJUIOQAAwEqEHAAAYCVCDgAAsBIhBwAAWImQAwAArETIAQAAViLkAAAAKxFyAACAlQg5AADASoQcAABgJUIOAACwEiEHAABYiZADAACsRMgBAABWIuQAAAArEXIAAICVCDkAAMBKhBwAAGAlQg4AALASIQcAAFiJkAMAAKxEyAEAAFYi5AAAACsRcgAAgJUIOQAAwEqEHAAAYKVWH3KWLVum3r17Ky4uTunp6dq9e3dLtwQAAL4FWnXIWbt2rXJycrRw4ULt27dPgwYNkt/vV01NTUu3BgAAWlirDjm/+tWv9Mgjj2jy5MlKTU3VypUr1bZtW/3mN79p6dYAAEALi23pBi5XQ0ODSkpKtGDBAmdddHS0MjIyFAgELrpNfX296uvrndd1dXWSpFAodMX7C9f/6YrvEwCA1uRqfL6ev19jzNfWtdqQ88UXX6ixsVFerzdivdfrVXl5+UW3ycvL09NPP33B+uTk5KvSIwAA17OEF67u/k+cOKGEhISvHG+1IedyLFiwQDk5Oc7rcDisY8eOqXPnzoqKilIoFFJycrKqqqrk8XhasNPrD3PfMpj3lsPctwzmveVcybk3xujEiRPq0aPH19a12pDTpUsXxcTEqLq6OmJ9dXW1kpKSLrqN2+2W2+2OWJeYmHhBncfj4R9/C2HuWwbz3nKY+5bBvLecKzX3X3cGp0mrvfDY5XIpLS1NxcXFzrpwOKzi4mL5fL4W7AwAAHwbtNozOZKUk5OjrKwsDRs2TCNGjNALL7ygU6dOafLkyS3dGgAAaGGtOuSMGzdOR48eVW5uroLBoAYPHqzCwsILLka+VG63WwsXLrzgV1q4+pj7lsG8txzmvmUw7y2nJeY+yvy1+68AAABaoVZ7TQ4AAMDXIeQAAAArEXIAAICVCDkAAMBKhJzzLFu2TL1791ZcXJzS09O1e/fulm6pVcvLy9Pw4cPVoUMHdevWTWPGjFFFRUVEzZkzZ5Sdna3OnTurffv2Gjt27AUPeKysrFRmZqbatm2rbt26ac6cOTp37ty1PJRWbdGiRYqKitKsWbOcdcz71fP555/rhz/8oTp37qz4+HgNHDhQe/fudcaNMcrNzVX37t0VHx+vjIwMHTx4MGIfx44d08SJE+XxeJSYmKgpU6bo5MmT1/pQWo3GxkY9+eSTSklJUXx8vP7mb/5G//qv/xrxvUbM+5WxY8cOff/731ePHj0UFRWlDRs2RIxfqXn+8MMPdccddyguLk7JyclavHjx5TVsYIwxZs2aNcblcpnf/OY35sCBA+aRRx4xiYmJprq6uqVba7X8fr959dVXTVlZmSktLTXf+973TK9evczJkyedmmnTppnk5GRTXFxs9u7da0aOHGluvfVWZ/zcuXNmwIABJiMjw7z//vvm7bffNl26dDELFixoiUNqdXbv3m169+5tbrnlFvPoo48665n3q+PYsWPmxhtvND/+8Y/Nrl27zKeffmq2bNli/vCHPzg1ixYtMgkJCWbDhg3mgw8+MP/4j/9oUlJSzOnTp52au+++2wwaNMi899575r//+79Nnz59zIQJE1rikFqFZ5991nTu3Nls2rTJHDp0yKxfv960b9/eLFmyxKlh3q+Mt99+2zz++OPmd7/7nZFk3njjjYjxKzHPdXV1xuv1mokTJ5qysjLz+uuvm/j4ePPSSy81u19Czv83YsQIk52d7bxubGw0PXr0MHl5eS3YlV1qamqMJLN9+3ZjjDG1tbWmTZs2Zv369U7Nxx9/bCSZQCBgjPm//1DR0dEmGAw6NStWrDAej8fU19df2wNoZU6cOGFuuukmU1RUZP72b//WCTnM+9Uzb948c/vtt3/leDgcNklJSea5555z1tXW1hq3221ef/11Y4wxH330kZFk9uzZ49Rs3rzZREVFmc8///zqNd+KZWZmmoceeihi3QMPPGAmTpxojGHer5a/DDlXap6XL19uOnbsGPGzZt68eaZv377N7pFfV0lqaGhQSUmJMjIynHXR0dHKyMhQIBBowc7sUldXJ0nq1KmTJKmkpERnz56NmPd+/fqpV69ezrwHAgENHDgw4gGPfr9foVBIBw4cuIbdtz7Z2dnKzMyMmF+Jeb+a3nrrLQ0bNkw/+MEP1K1bNw0ZMkS//vWvnfFDhw4pGAxGzH1CQoLS09Mj5j4xMVHDhg1zajIyMhQdHa1du3Zdu4NpRW699VYVFxfrk08+kSR98MEHevfdd3XPPfdIYt6vlSs1z4FAQHfeeadcLpdT4/f7VVFRoePHjzerp1b9xOMr5YsvvlBjY+MFT0r2er0qLy9voa7sEg6HNWvWLN12220aMGCAJCkYDMrlcl3wJaler1fBYNCpudjfS9MYLm7NmjXat2+f9uzZc8EY8371fPrpp1qxYoVycnL02GOPac+ePfrpT38ql8ulrKwsZ+4uNrfnz323bt0ixmNjY9WpUyfm/ivMnz9foVBI/fr1U0xMjBobG/Xss89q4sSJksS8XyNXap6DwaBSUlIu2EfTWMeOHS+5J0IOrons7GyVlZXp3XffbelWrFdVVaVHH31URUVFiouLa+l2rivhcFjDhg3TL3/5S0nSkCFDVFZWppUrVyorK6uFu7PXunXr9Nprr2n16tW6+eabVVpaqlmzZqlHjx7M+3WOX1dJ6tKli2JiYi64u6S6ulpJSUkt1JU9ZsyYoU2bNumdd95Rz549nfVJSUlqaGhQbW1tRP35856UlHTRv5emMVyopKRENTU1Gjp0qGJjYxUbG6vt27frxRdfVGxsrLxeL/N+lXTv3l2pqakR6/r376/KykpJf567r/tZk5SUpJqamojxc+fO6dixY8z9V5gzZ47mz5+v8ePHa+DAgZo0aZJmz56tvLw8Scz7tXKl5vlK/vwh5EhyuVxKS0tTcXGxsy4cDqu4uFg+n68FO2vdjDGaMWOG3njjDW3duvWC049paWlq06ZNxLxXVFSosrLSmXefz6f9+/dH/KcoKiqSx+O54MME/+euu+7S/v37VVpa6izDhg3TxIkTnT8z71fHbbfddsFjEj755BPdeOONkqSUlBQlJSVFzH0oFNKuXbsi5r62tlYlJSVOzdatWxUOh5Wenn4NjqL1+dOf/qTo6MiPs5iYGIXDYUnM+7VypebZ5/Npx44dOnv2rFNTVFSkvn37NutXVZK4hbzJmjVrjNvtNvn5+eajjz4yU6dONYmJiRF3l6B5pk+fbhISEsy2bdvMkSNHnOVPf/qTUzNt2jTTq1cvs3XrVrN3717j8/mMz+dzxptuZR49erQpLS01hYWFpmvXrtzK3Ezn311lDPN+tezevdvExsaaZ5991hw8eNC89tprpm3btua3v/2tU7No0SKTmJho3nzzTfPhhx+a++6776K32A4ZMsTs2rXLvPvuu+amm27iVuavkZWVZW644QbnFvLf/e53pkuXLmbu3LlODfN+ZZw4ccK8//775v333zeSzK9+9Svz/vvvm//93/81xlyZea6trTVer9dMmjTJlJWVmTVr1pi2bdtyC/k39Z//+Z+mV69exuVymREjRpj33nuvpVtq1SRddHn11VedmtOnT5uf/OQnpmPHjqZt27bm/vvvN0eOHInYz2effWbuueceEx8fb7p06WJ+9rOfmbNnz17jo2nd/jLkMO9Xz8aNG82AAQOM2+02/fr1My+//HLEeDgcNk8++aTxer3G7Xabu+66y1RUVETUfPnll2bChAmmffv2xuPxmMmTJ5sTJ05cy8NoVUKhkHn00UdNr169TFxcnPnOd75jHn/88YhbkJn3K+Odd9656M/1rKwsY8yVm+cPPvjA3H777cbtdpsbbrjBLFq06LL6jTLmvEdCAgAAWIJrcgAAgJUIOQAAwEqEHAAAYCVCDgAAsBIhBwAAWImQAwAArETIAQAAViLkAAAAKxFyAACAlQg5AADASoQcAABgJUIOAACw0v8DUMldF8cXkJYAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(length_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73266915",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_token = [text_to_word_sequence(_) for _ in X_train.astype(str)]\n",
    "X_val_token = [text_to_word_sequence(_) for _ in X_val.astype(str)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "efcd329c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learn embedding representation of words in reviews\n",
    "word2vec = Word2Vec(sentences=X_train_token, vector_size=50, min_count=10) #Reduced vector size from 100 to 50 and min_count from 5 to 10\n",
    "# Store words and trained embeddings in wv\n",
    "wv = word2vec.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c462448",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hardly', 0.6714343428611755),\n",
       " ('barely', 0.602747917175293),\n",
       " ('though', 0.5431169271469116),\n",
       " ('yes', 0.4965512752532959),\n",
       " ('never', 0.48427829146385193),\n",
       " ('neither', 0.4770137071609497),\n",
       " ('nothing', 0.46469846367836),\n",
       " ('that', 0.46058890223503113),\n",
       " ('noti', 0.4463513493537903),\n",
       " ('pretty', 0.439453661441803)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar(\"not\", topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fbd8b6c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33128"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wv.key_to_index.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657f7bb0",
   "metadata": {},
   "source": [
    "# Optimized Embedding Function v2 - on disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af0fefc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "166f1bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sentence_batch_to_disk(wv_vectors, wv_vocab, vector_size, sentences_batch, batch_index, output_dir):\n",
    "    \"\"\"\n",
    "    Processes a sentence batch and saves the result to disk using pickle.\n",
    "    \"\"\"\n",
    "    batch_embeddings = []\n",
    "\n",
    "    for sentence in sentences_batch:\n",
    "        valid_words = [word for word in sentence if word in wv_vocab]\n",
    "        if valid_words:\n",
    "            embeddings = np.array([wv_vectors[word] for word in valid_words])\n",
    "        else:\n",
    "            embeddings = np.array([]).reshape(0, vector_size)\n",
    "\n",
    "        batch_embeddings.append(embeddings)\n",
    "\n",
    "    # Save batch to disk\n",
    "    file_path = os.path.join(output_dir, f'batch_{batch_index}.pkl')\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(batch_embeddings, f)\n",
    "\n",
    "    return file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "122f221a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_optimized_to_disk(wv, sentences, batch_size=10, n_jobs=-1, output_dir='embeddings_batches'):\n",
    "    \"\"\"\n",
    "    Optimized embedding function that saves intermediate results to disk to avoid memory overload.\n",
    "    \"\"\"\n",
    "    print(f\"Processing {len(sentences)} sentences...\")\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    vocab = set(wv.key_to_index.keys())\n",
    "    vector_size = wv.vector_size\n",
    "    batches = [sentences[i:i + batch_size] for i in range(0, len(sentences), batch_size)]\n",
    "\n",
    "    print(f\"Created {len(batches)} batches of ~{batch_size} sentences each\")\n",
    "    print(f\"Using {n_jobs} parallel processes...\")\n",
    "\n",
    "    # Process and store results to disk in parallel\n",
    "    result_files = Parallel(n_jobs=n_jobs, verbose=1, backend='loky')(\n",
    "        delayed(process_sentence_batch_to_disk)(wv, vocab, vector_size, batch, idx, output_dir)\n",
    "        for idx, batch in enumerate(batches)\n",
    "    )\n",
    "\n",
    "    print(f\"Embeddings saved to disk at '{output_dir}'. Total: {len(result_files)} files.\")\n",
    "    return result_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "73737d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_embeddings_from_disk(file_paths):\n",
    "    \"\"\"\n",
    "    Load and concatenate embeddings from disk.\n",
    "    \"\"\"\n",
    "    all_embeddings = []\n",
    "    for path in file_paths:\n",
    "        with open(path, 'rb') as f:\n",
    "            all_embeddings.extend(pickle.load(f))\n",
    "    return all_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "37426c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 288000 sentences...\n",
      "Created 5760 batches of ~50 sentences each\n",
      "Using -1 parallel processes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:    4.1s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:    8.6s\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:   15.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed:   24.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1896 tasks      | elapsed:   32.5s\n",
      "[Parallel(n_jobs=-1)]: Done 3196 tasks      | elapsed:   46.7s\n",
      "[Parallel(n_jobs=-1)]: Done 5032 tasks      | elapsed:  1.1min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings saved to disk at 'embeddings_batches'. Total: 5760 files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 5760 out of 5760 | elapsed:  1.2min finished\n"
     ]
    }
   ],
   "source": [
    "file_paths = embedding_optimized_to_disk(wv, X_train_token, batch_size=50, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ac7b53e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_embedded = load_all_embeddings_from_disk(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b8d0fe10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "288000"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train_embedded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fe5456",
   "metadata": {},
   "source": [
    "# Optimized Embedding Function v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cf20318c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1579a4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sentence_batch(wv_vectors, wv_vocab, vector_size, sentences_batch):\n",
    "    \"\"\"\n",
    "    Process a batch of sentences efficiently.\n",
    "\n",
    "    Args:\n",
    "        wv_vectors: Word2Vec vectors array\n",
    "        wv_vocab: Set of vocabulary words (for O(1) lookup)\n",
    "        vector_size: Dimension of word vectors\n",
    "        sentences_batch: List of sentences to process\n",
    "\n",
    "    Returns:\n",
    "        List of numpy arrays (embeddings for each sentence)\n",
    "    \"\"\"\n",
    "    batch_embeddings = []\n",
    "\n",
    "    for sentence in sentences_batch:\n",
    "        # Filter valid words using set lookup (O(1))\n",
    "        valid_words = [word for word in sentence if word in wv_vocab]\n",
    "\n",
    "        if valid_words:\n",
    "            # Get embeddings for valid words\n",
    "            embeddings = np.array([wv_vectors[word] for word in valid_words])\n",
    "        else:\n",
    "            # Empty sentence or no valid words\n",
    "            embeddings = np.array([]).reshape(0, vector_size)\n",
    "\n",
    "        batch_embeddings.append(embeddings)\n",
    "\n",
    "    return batch_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12cc68dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_optimized(wv, sentences, batch_size=10, n_jobs=-1):\n",
    "    \"\"\"\n",
    "    Optimized embedding function using joblib parallelization.\n",
    "\n",
    "    Args:\n",
    "        wv: Word2Vec vectors object\n",
    "        sentences: List of tokenized sentences\n",
    "        batch_size: Number of sentences per batch\n",
    "        n_jobs: Number of parallel jobs (-1 for all cores)\n",
    "\n",
    "    Returns:\n",
    "        List of numpy arrays (embeddings for each sentence)\n",
    "    \"\"\"\n",
    "    print(f\"Processing {len(sentences)} sentences...\")\n",
    "\n",
    "    # Pre-compute vocabulary set for O(1) lookups\n",
    "    vocab = set(wv.key_to_index.keys())\n",
    "    vector_size = wv.vector_size\n",
    "\n",
    "    # Split sentences into batches\n",
    "    batches = [sentences[i:i + batch_size]\n",
    "               for i in range(0, len(sentences), batch_size)]\n",
    "\n",
    "    print(f\"Created {len(batches)} batches of ~{batch_size} sentences each\")\n",
    "    print(f\"Using {n_jobs} parallel processes...\")\n",
    "\n",
    "    # Process batches in parallel\n",
    "    results = Parallel(n_jobs=n_jobs, verbose=1, backend='loky')(\n",
    "        delayed(process_sentence_batch)(wv, vocab, vector_size, batch)\n",
    "        for batch in batches\n",
    "    )\n",
    "\n",
    "    # Flatten results\n",
    "    all_embeddings = []\n",
    "    for batch_result in results:\n",
    "        all_embeddings.extend(batch_result)\n",
    "\n",
    "    print(f\"Successfully processed {len(all_embeddings)} sentences\")\n",
    "    return all_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a22fde94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 288000 sentences...\n",
      "Created 28800 batches of ~10 sentences each\n",
      "Using -1 parallel processes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    3.2s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:    9.6s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:   20.2s\n",
      "[Parallel(n_jobs=-1)]: Done 1088 tasks      | elapsed:   32.8s\n",
      "[Parallel(n_jobs=-1)]: Done 2008 tasks      | elapsed:   49.8s\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "X_train_embedded = embedding_optimized(wv, X_train_token, batch_size=10, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f257c922",
   "metadata": {},
   "source": [
    "# Not-Optimized Embedding Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c21e66ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88769288",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_sentence(word2vec, sentence):\n",
    "    wv = word2vec.wv\n",
    "    res_matrix = []\n",
    "\n",
    "    for word in sentence:\n",
    "\n",
    "        if word in wv:\n",
    "            res_matrix.append(wv[word])\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    return np.array(res_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "993ee87d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "106"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train_token[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "86a8ccde",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checks\n",
    "embedded_sentence = embed_sentence(word2vec, X_train_token[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "19e20516",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(embedded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e94a8dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(type(embedded_sentence) == np.ndarray)\n",
    "#assert(embedded_sentence.shape == (120, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894166e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def embedding(word2vec, sentences):\n",
    "\n",
    "    sentences_matrix = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentences_matrix.append(embed_sentence(word2vec, sentence))\n",
    "\n",
    "    return sentences_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eee104a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_embedded = embedding(word2vec, X_train_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e91d3879",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train_embedded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3680129e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train_token[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac735d56",
   "metadata": {},
   "source": [
    "# Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f4ffd5",
   "metadata": {},
   "source": [
    "from tensorflow.keras.utils import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3b872a20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train_embedded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e3cc49d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_padded_batches(X_embedded, batch_size, maxlen, save_path):\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    for i in range(0, len(X_embedded), batch_size):\n",
    "        batch = X_embedded[i:i+batch_size]\n",
    "        padded = pad_sequences(batch, maxlen=maxlen, padding='post', dtype='float32')\n",
    "        np.save(os.path.join(save_path, f\"batch_{i//batch_size}.npy\"), padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cd68511d",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_padded_batches(X_train_embedded, batch_size=100, maxlen=400, save_path=\"padded_embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aa494c6c",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 21.5 GiB for an array with shape (288000, 400, 50) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m X_train_pad_s \u001b[38;5;241m=\u001b[39m \u001b[43mpad_sequences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_embedded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfloat32\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxlen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m400\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/sentiscope/lib/python3.10/site-packages/keras/src/utils/sequence_utils.py:113\u001b[0m, in \u001b[0;36mpad_sequences\u001b[0;34m(sequences, maxlen, dtype, padding, truncating, value)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dtype_str:\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    108\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`dtype` \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not compatible with `value`\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms type: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    109\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mYou should set `dtype=object` for variable length \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    110\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    111\u001b[0m     )\n\u001b[0;32m--> 113\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxlen\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msample_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(sequences):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[0;32m~/.pyenv/versions/sentiscope/lib/python3.10/site-packages/numpy/core/numeric.py:329\u001b[0m, in \u001b[0;36mfull\u001b[0;34m(shape, fill_value, dtype, order, like)\u001b[0m\n\u001b[1;32m    327\u001b[0m     fill_value \u001b[38;5;241m=\u001b[39m asarray(fill_value)\n\u001b[1;32m    328\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m fill_value\u001b[38;5;241m.\u001b[39mdtype\n\u001b[0;32m--> 329\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    330\u001b[0m multiarray\u001b[38;5;241m.\u001b[39mcopyto(a, fill_value, casting\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munsafe\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m a\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 21.5 GiB for an array with shape (288000, 400, 50) and data type float32"
     ]
    }
   ],
   "source": [
    "X_train_pad_s = pad_sequences(X_train_embedded, dtype=\"float32\", padding='post', value=0, maxlen=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27eac522",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_pad_s = pad_sequences(X_val_embedded, dtype=\"float32\", padding='post', value=0, maxlen=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentiscope",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
