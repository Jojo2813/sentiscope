{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ba274863",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "from gensim.models import Word2Vec\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6a266057",
   "metadata": {},
   "outputs": [],
   "source": [
    "path= \"../raw_data/train_df_ml_clean.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ee9c544c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e5be3665",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "360000"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df = clean_df.sample(frac=0.1)\n",
    "len(clean_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b7662e29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1459873</th>\n",
       "      <td>0</td>\n",
       "      <td>Good idea poorly executed: The Delphi SkyFi3 i...</td>\n",
       "      <td>good idea poorly executed the delphi skyfi3 is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3183647</th>\n",
       "      <td>1</td>\n",
       "      <td>the sims 2, a great new game!: i think that th...</td>\n",
       "      <td>the sims a great new game i think that the sim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1802979</th>\n",
       "      <td>0</td>\n",
       "      <td>Give me a break: Maybe this book could have be...</td>\n",
       "      <td>give me a break maybe this book could have bee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102865</th>\n",
       "      <td>1</td>\n",
       "      <td>Very good work, by a brilliant cricket writer:...</td>\n",
       "      <td>very good work by a brilliant cricket writer i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000720</th>\n",
       "      <td>0</td>\n",
       "      <td>Am I in another dimension?: I want my money ba...</td>\n",
       "      <td>am i in another dimension i want my money back...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         label                                               text  \\\n",
       "1459873      0  Good idea poorly executed: The Delphi SkyFi3 i...   \n",
       "3183647      1  the sims 2, a great new game!: i think that th...   \n",
       "1802979      0  Give me a break: Maybe this book could have be...   \n",
       "102865       1  Very good work, by a brilliant cricket writer:...   \n",
       "1000720      0  Am I in another dimension?: I want my money ba...   \n",
       "\n",
       "                                                clean_text  \n",
       "1459873  good idea poorly executed the delphi skyfi3 is...  \n",
       "3183647  the sims a great new game i think that the sim...  \n",
       "1802979  give me a break maybe this book could have bee...  \n",
       "102865   very good work by a brilliant cricket writer i...  \n",
       "1000720  am i in another dimension i want my money back...  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3194c0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = clean_df['clean_text'].to_numpy()\n",
    "y = clean_df['label'].to_numpy()\n",
    "\n",
    "val_size= 0.2\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=val_size, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1d2c95b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(288000, 72000)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train), len(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e8514ff4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['cheap and annoying i broke most of them trying to get them to stay on my speaker cable because they would slip off if not crimped hard enough save your money and buy better connector or just use the bare wire',\n",
       "       'a classic crichton made all the science thought and suspense get thrown into one book amazing i loved how he described piedmont it wa so realistic that i could put a detailed picture in my head of how it really would be it wa a great book i finished it in two day i wa glued i couldnt stop reading because i wanted to know what happened next and what they did to stop this and how they did that it wa great i would reccomend it',\n",
       "       'the replacement mirror sent vibrates in housing just like the 1st mirror the fit system replacement mirror sent to replace the the 1st fit system mirror still vibrates in the mirror housing at hiway speed the vibration is the same on the replacement mirror a the 1st mirrorafter talking to a technition at fit system he told me i could make a adjustment on the mirror to tighten the mirror in the housing to eliminate the vibrationthis adjustment requires removing the from the car and reinstalling it and i have not had time to do this a yetmy question to fit system is why they did not ship me a mirror that wa correctly adjustedi am not very happy with this product but i will try to make it work',\n",
       "       'dont buy pack the product itself performs satisfactory but beware of the seller sending only bottle instead of the promised after complaining they sent a 2nd bottleand after complaining again i wa refunded to my account for the never received 3rd bottle i dont believe a pack exists but the seller assumes you will not bother to complain about getting only bottle despite paying they were wrong in my case hope this help',\n",
       "       'terrible in every way this product is absolutely horrible for two reason the quality is extremely cheap making adjustment nearly impossible the key and tension lifter used to adjust the part that fit into your car cup holder is made of very thin plastic it break easily but it is so hard to move you practically have to break it to operate it the second reason is design the large wide semicircular section cover up anything beside the holder youre trying to create so if you have two existing cup holder in your car and youre trying to use this one to improve upon the size of one of them the whole point of this thing right then you have to sacrifice the other cup holder to do it ridiculous glad i didnt spend much i got less than i paid for if thats even possible',\n",
       "       'excellent book but with fault the glorious food of greece is an excellent introduction to the superb cuisine of that ancient land and a the title suggests the subject is treated with all the gusto and enthusiasm that it deserves however a is often the case with those affected by an extreme enthusiasm for a particular subject the author may be tending toward too narrow and rigid concept of what constitutes authentic greek cooking greek cooking not only varies from region to region but even the same dish may recieve a different treatment dependent on family tradition keeping that in mind it best to just enjoy that which is set before u for itsown worth without so much forced reference to supposed standard of authenticity',\n",
       "       'dont order from brybelly for this dice cup do not order it through brybelly they will send u a totally different cup instead of this one i wanted this style of cup and received something completely different they would take it back but at the loss of my shipping cost do not order with brybelly',\n",
       "       'terrible this episode wa the longest i ever saw the singing wa bad and uninteresting i thought it woudl never end i love buffy but man that show wa not meant to be a musical',\n",
       "       'a miracle cleaner when i purchased my home two year ago my toilet had a ring i have tried various product in a effort to remove the ring eventually my best friend told me that i would probably have to replace the toilet because it would be impossible to remove the ring one day i decided to google my problem i came up with several link that suggested the pumie i read the review on pumie which were great btw i decided to order the product after about minute of wiping the ring my toilet wa a good a new',\n",
       "       'wsj made a mistake im not going to buy the wsj now on the kindle only bc of the pricing issuesi figure even though i am a conscientious consumer and i care about the earth and therefore like the idea of digital print at the expense of my hard earned dollar i do not playi think ill just go to starbucks more frequently and pick up one off of a table and read it second hand so the company actually loses my money a a result of their foolish pricing'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9514611c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b5d4bab2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[776,\n",
       " 261,\n",
       " 261,\n",
       " 464,\n",
       " 482,\n",
       " 836,\n",
       " 362,\n",
       " 643,\n",
       " 139,\n",
       " 193,\n",
       " 260,\n",
       " 745,\n",
       " 126,\n",
       " 185,\n",
       " 197,\n",
       " 732,\n",
       " 529,\n",
       " 271,\n",
       " 717,\n",
       " 374,\n",
       " 122,\n",
       " 404,\n",
       " 105,\n",
       " 288,\n",
       " 496,\n",
       " 405,\n",
       " 437,\n",
       " 581,\n",
       " 229,\n",
       " 377,\n",
       " 450,\n",
       " 644,\n",
       " 122,\n",
       " 612,\n",
       " 920,\n",
       " 414,\n",
       " 206,\n",
       " 574,\n",
       " 844,\n",
       " 196,\n",
       " 362,\n",
       " 522,\n",
       " 582,\n",
       " 895,\n",
       " 217,\n",
       " 868,\n",
       " 337,\n",
       " 216,\n",
       " 517,\n",
       " 441,\n",
       " 618,\n",
       " 252,\n",
       " 218,\n",
       " 378,\n",
       " 213,\n",
       " 376,\n",
       " 126,\n",
       " 527,\n",
       " 659,\n",
       " 206,\n",
       " 154,\n",
       " 165,\n",
       " 368,\n",
       " 192,\n",
       " 138,\n",
       " 560,\n",
       " 156,\n",
       " 670,\n",
       " 933,\n",
       " 725,\n",
       " 899,\n",
       " 609,\n",
       " 416,\n",
       " 168,\n",
       " 533,\n",
       " 344,\n",
       " 147,\n",
       " 297,\n",
       " 301,\n",
       " 611,\n",
       " 539,\n",
       " 235,\n",
       " 175,\n",
       " 160,\n",
       " 296,\n",
       " 356,\n",
       " 398,\n",
       " 479,\n",
       " 827,\n",
       " 358,\n",
       " 146,\n",
       " 937,\n",
       " 855,\n",
       " 219,\n",
       " 465,\n",
       " 174,\n",
       " 609,\n",
       " 402,\n",
       " 197,\n",
       " 517,\n",
       " 103,\n",
       " 143,\n",
       " 324,\n",
       " 690,\n",
       " 246,\n",
       " 287,\n",
       " 241,\n",
       " 786,\n",
       " 511,\n",
       " 587,\n",
       " 358,\n",
       " 223,\n",
       " 468,\n",
       " 568,\n",
       " 191,\n",
       " 925,\n",
       " 803,\n",
       " 288,\n",
       " 607,\n",
       " 728,\n",
       " 325,\n",
       " 565,\n",
       " 186,\n",
       " 579,\n",
       " 469,\n",
       " 291,\n",
       " 167,\n",
       " 369,\n",
       " 139,\n",
       " 208,\n",
       " 487,\n",
       " 165,\n",
       " 538,\n",
       " 473,\n",
       " 197,\n",
       " 441,\n",
       " 453,\n",
       " 863,\n",
       " 134,\n",
       " 188,\n",
       " 222,\n",
       " 285,\n",
       " 897,\n",
       " 136,\n",
       " 262,\n",
       " 747,\n",
       " 679,\n",
       " 317,\n",
       " 130,\n",
       " 326,\n",
       " 290,\n",
       " 373,\n",
       " 196,\n",
       " 580,\n",
       " 369,\n",
       " 306,\n",
       " 711,\n",
       " 492,\n",
       " 565,\n",
       " 920,\n",
       " 418,\n",
       " 573,\n",
       " 310,\n",
       " 660,\n",
       " 102,\n",
       " 773,\n",
       " 222,\n",
       " 242,\n",
       " 145,\n",
       " 166,\n",
       " 564,\n",
       " 307,\n",
       " 225,\n",
       " 97,\n",
       " 408,\n",
       " 631,\n",
       " 521,\n",
       " 349,\n",
       " 147,\n",
       " 145,\n",
       " 676,\n",
       " 785,\n",
       " 248,\n",
       " 547,\n",
       " 650,\n",
       " 840,\n",
       " 221,\n",
       " 246,\n",
       " 371,\n",
       " 716,\n",
       " 356,\n",
       " 897,\n",
       " 109,\n",
       " 474,\n",
       " 178,\n",
       " 149,\n",
       " 152,\n",
       " 555,\n",
       " 569,\n",
       " 631,\n",
       " 697,\n",
       " 396,\n",
       " 347,\n",
       " 289,\n",
       " 555,\n",
       " 940,\n",
       " 643,\n",
       " 421,\n",
       " 646,\n",
       " 317,\n",
       " 163,\n",
       " 161,\n",
       " 136,\n",
       " 276,\n",
       " 162,\n",
       " 261,\n",
       " 193,\n",
       " 334,\n",
       " 569,\n",
       " 177,\n",
       " 524,\n",
       " 265,\n",
       " 747,\n",
       " 149,\n",
       " 183,\n",
       " 178,\n",
       " 231,\n",
       " 402,\n",
       " 742,\n",
       " 292,\n",
       " 146,\n",
       " 197,\n",
       " 172,\n",
       " 409,\n",
       " 344,\n",
       " 143,\n",
       " 202,\n",
       " 166,\n",
       " 553,\n",
       " 247,\n",
       " 150,\n",
       " 275,\n",
       " 172,\n",
       " 265,\n",
       " 175,\n",
       " 103,\n",
       " 389,\n",
       " 863,\n",
       " 524,\n",
       " 556,\n",
       " 263,\n",
       " 389,\n",
       " 368,\n",
       " 256,\n",
       " 575,\n",
       " 755,\n",
       " 277,\n",
       " 174,\n",
       " 449,\n",
       " 595,\n",
       " 132,\n",
       " 487,\n",
       " 178,\n",
       " 522,\n",
       " 170,\n",
       " 369,\n",
       " 198,\n",
       " 154,\n",
       " 616,\n",
       " 400,\n",
       " 263,\n",
       " 289,\n",
       " 464,\n",
       " 127,\n",
       " 299,\n",
       " 140,\n",
       " 413,\n",
       " 843,\n",
       " 152,\n",
       " 595,\n",
       " 463,\n",
       " 192,\n",
       " 479,\n",
       " 287,\n",
       " 426,\n",
       " 420,\n",
       " 124,\n",
       " 876,\n",
       " 526,\n",
       " 108,\n",
       " 914,\n",
       " 463,\n",
       " 949,\n",
       " 937,\n",
       " 232,\n",
       " 142,\n",
       " 844,\n",
       " 112,\n",
       " 266,\n",
       " 248,\n",
       " 638,\n",
       " 141,\n",
       " 298,\n",
       " 119,\n",
       " 103,\n",
       " 654,\n",
       " 494,\n",
       " 776,\n",
       " 531,\n",
       " 940,\n",
       " 305,\n",
       " 128,\n",
       " 670,\n",
       " 420,\n",
       " 763,\n",
       " 351,\n",
       " 955,\n",
       " 582,\n",
       " 456,\n",
       " 469,\n",
       " 854,\n",
       " 738,\n",
       " 157,\n",
       " 616,\n",
       " 493,\n",
       " 478,\n",
       " 458,\n",
       " 168,\n",
       " 251,\n",
       " 853,\n",
       " 163,\n",
       " 882,\n",
       " 742,\n",
       " 538,\n",
       " 715,\n",
       " 211,\n",
       " 379,\n",
       " 842,\n",
       " 289,\n",
       " 410,\n",
       " 225,\n",
       " 614,\n",
       " 166,\n",
       " 421,\n",
       " 134,\n",
       " 167,\n",
       " 593,\n",
       " 434,\n",
       " 405,\n",
       " 350,\n",
       " 711,\n",
       " 176,\n",
       " 132,\n",
       " 864,\n",
       " 638,\n",
       " 672,\n",
       " 927,\n",
       " 246,\n",
       " 556,\n",
       " 306,\n",
       " 287,\n",
       " 707,\n",
       " 411,\n",
       " 685,\n",
       " 205,\n",
       " 142,\n",
       " 448,\n",
       " 844,\n",
       " 535,\n",
       " 750,\n",
       " 733,\n",
       " 371,\n",
       " 301,\n",
       " 570,\n",
       " 357,\n",
       " 513,\n",
       " 538,\n",
       " 126,\n",
       " 535,\n",
       " 623,\n",
       " 127,\n",
       " 523,\n",
       " 325,\n",
       " 524,\n",
       " 443,\n",
       " 572,\n",
       " 710,\n",
       " 398,\n",
       " 271,\n",
       " 624,\n",
       " 867,\n",
       " 578,\n",
       " 335,\n",
       " 379,\n",
       " 162,\n",
       " 413,\n",
       " 145,\n",
       " 221,\n",
       " 550,\n",
       " 437,\n",
       " 358,\n",
       " 179,\n",
       " 399,\n",
       " 369,\n",
       " 492,\n",
       " 409,\n",
       " 620,\n",
       " 220,\n",
       " 442,\n",
       " 604,\n",
       " 287,\n",
       " 338,\n",
       " 345,\n",
       " 142,\n",
       " 113,\n",
       " 124,\n",
       " 480,\n",
       " 210,\n",
       " 648,\n",
       " 228,\n",
       " 490,\n",
       " 864,\n",
       " 128,\n",
       " 324,\n",
       " 130,\n",
       " 151,\n",
       " 279,\n",
       " 213,\n",
       " 372,\n",
       " 917,\n",
       " 263,\n",
       " 312,\n",
       " 539,\n",
       " 798,\n",
       " 494,\n",
       " 961,\n",
       " 624,\n",
       " 190,\n",
       " 570,\n",
       " 240,\n",
       " 643,\n",
       " 507,\n",
       " 111,\n",
       " 659,\n",
       " 502,\n",
       " 425,\n",
       " 928,\n",
       " 736,\n",
       " 498,\n",
       " 295,\n",
       " 461,\n",
       " 421,\n",
       " 241,\n",
       " 279,\n",
       " 487,\n",
       " 711,\n",
       " 603,\n",
       " 237,\n",
       " 194,\n",
       " 667,\n",
       " 140,\n",
       " 343,\n",
       " 441,\n",
       " 364,\n",
       " 140,\n",
       " 160,\n",
       " 393,\n",
       " 236,\n",
       " 227,\n",
       " 581,\n",
       " 625,\n",
       " 241,\n",
       " 636,\n",
       " 307,\n",
       " 477,\n",
       " 191,\n",
       " 196,\n",
       " 756,\n",
       " 527,\n",
       " 617,\n",
       " 334,\n",
       " 806,\n",
       " 171,\n",
       " 200,\n",
       " 110,\n",
       " 177,\n",
       " 850,\n",
       " 915,\n",
       " 332,\n",
       " 623,\n",
       " 365,\n",
       " 822,\n",
       " 174,\n",
       " 468,\n",
       " 534,\n",
       " 199,\n",
       " 477,\n",
       " 133,\n",
       " 307,\n",
       " 226,\n",
       " 341,\n",
       " 730,\n",
       " 508,\n",
       " 325,\n",
       " 127,\n",
       " 148,\n",
       " 196,\n",
       " 143,\n",
       " 914,\n",
       " 445,\n",
       " 556,\n",
       " 502,\n",
       " 343,\n",
       " 510,\n",
       " 127,\n",
       " 184,\n",
       " 891,\n",
       " 898,\n",
       " 521,\n",
       " 160,\n",
       " 306,\n",
       " 205,\n",
       " 619,\n",
       " 923,\n",
       " 416,\n",
       " 497,\n",
       " 711,\n",
       " 569,\n",
       " 234,\n",
       " 279,\n",
       " 185,\n",
       " 414,\n",
       " 118,\n",
       " 321,\n",
       " 456,\n",
       " 440,\n",
       " 168,\n",
       " 628,\n",
       " 319,\n",
       " 433,\n",
       " 232,\n",
       " 150,\n",
       " 728,\n",
       " 582,\n",
       " 801,\n",
       " 623,\n",
       " 757,\n",
       " 522,\n",
       " 428,\n",
       " 557,\n",
       " 560,\n",
       " 96,\n",
       " 268,\n",
       " 743,\n",
       " 635,\n",
       " 185,\n",
       " 126,\n",
       " 608,\n",
       " 300,\n",
       " 551,\n",
       " 216,\n",
       " 233,\n",
       " 335,\n",
       " 198,\n",
       " 572,\n",
       " 244,\n",
       " 241,\n",
       " 350,\n",
       " 824,\n",
       " 551,\n",
       " 92,\n",
       " 143,\n",
       " 264,\n",
       " 291,\n",
       " 445,\n",
       " 600,\n",
       " 488,\n",
       " 428,\n",
       " 275,\n",
       " 814,\n",
       " 492,\n",
       " 563,\n",
       " 300,\n",
       " 233,\n",
       " 468,\n",
       " 613,\n",
       " 149,\n",
       " 566,\n",
       " 919,\n",
       " 214,\n",
       " 833,\n",
       " 293,\n",
       " 579,\n",
       " 717,\n",
       " 314,\n",
       " 538,\n",
       " 747,\n",
       " 389,\n",
       " 469,\n",
       " 166,\n",
       " 141,\n",
       " 421,\n",
       " 974,\n",
       " 283,\n",
       " 729,\n",
       " 651,\n",
       " 317,\n",
       " 131,\n",
       " 293,\n",
       " 706,\n",
       " 558,\n",
       " 115,\n",
       " 733,\n",
       " 779,\n",
       " 218,\n",
       " 157,\n",
       " 561,\n",
       " 739,\n",
       " 924,\n",
       " 764,\n",
       " 588,\n",
       " 168,\n",
       " 950,\n",
       " 237,\n",
       " 148,\n",
       " 279,\n",
       " 264,\n",
       " 695,\n",
       " 304,\n",
       " 407,\n",
       " 283,\n",
       " 758,\n",
       " 436,\n",
       " 493,\n",
       " 489,\n",
       " 335,\n",
       " 696,\n",
       " 393,\n",
       " 428,\n",
       " 678,\n",
       " 557,\n",
       " 158,\n",
       " 139,\n",
       " 431,\n",
       " 841,\n",
       " 200,\n",
       " 956,\n",
       " 369,\n",
       " 616,\n",
       " 495,\n",
       " 534,\n",
       " 423,\n",
       " 497,\n",
       " 601,\n",
       " 286,\n",
       " 371,\n",
       " 772,\n",
       " 126,\n",
       " 356,\n",
       " 444,\n",
       " 304,\n",
       " 544,\n",
       " 660,\n",
       " 340,\n",
       " 107,\n",
       " 462,\n",
       " 316,\n",
       " 788,\n",
       " 476,\n",
       " 338,\n",
       " 382,\n",
       " 313,\n",
       " 797,\n",
       " 899,\n",
       " 218,\n",
       " 123,\n",
       " 134,\n",
       " 467,\n",
       " 114,\n",
       " 435,\n",
       " 770,\n",
       " 652,\n",
       " 381,\n",
       " 712,\n",
       " 100,\n",
       " 502,\n",
       " 234,\n",
       " 295,\n",
       " 98,\n",
       " 653,\n",
       " 513,\n",
       " 401,\n",
       " 332,\n",
       " 95,\n",
       " 555,\n",
       " 829,\n",
       " 888,\n",
       " 455,\n",
       " 178,\n",
       " 490,\n",
       " 316,\n",
       " 172,\n",
       " 284,\n",
       " 577,\n",
       " 320,\n",
       " 646,\n",
       " 353,\n",
       " 821,\n",
       " 173,\n",
       " 276,\n",
       " 718,\n",
       " 373,\n",
       " 131,\n",
       " 281,\n",
       " 439,\n",
       " 291,\n",
       " 466,\n",
       " 732,\n",
       " 610,\n",
       " 208,\n",
       " 262,\n",
       " 867,\n",
       " 139,\n",
       " 565,\n",
       " 786,\n",
       " 330,\n",
       " 109,\n",
       " 396,\n",
       " 401,\n",
       " 318,\n",
       " 525,\n",
       " 109,\n",
       " 761,\n",
       " 771,\n",
       " 295,\n",
       " 302,\n",
       " 737,\n",
       " 189,\n",
       " 812,\n",
       " 504,\n",
       " 164,\n",
       " 166,\n",
       " 342,\n",
       " 890,\n",
       " 392,\n",
       " 687,\n",
       " 650,\n",
       " 624,\n",
       " 234,\n",
       " 895,\n",
       " 862,\n",
       " 269,\n",
       " 454,\n",
       " 251,\n",
       " 363,\n",
       " 190,\n",
       " 311,\n",
       " 540,\n",
       " 749,\n",
       " 578,\n",
       " 852,\n",
       " 447,\n",
       " 758,\n",
       " 419,\n",
       " 857,\n",
       " 413,\n",
       " 523,\n",
       " 445,\n",
       " 557,\n",
       " 794,\n",
       " 161,\n",
       " 134,\n",
       " 425,\n",
       " 229,\n",
       " 466,\n",
       " 133,\n",
       " 272,\n",
       " 204,\n",
       " 145,\n",
       " 601,\n",
       " 333,\n",
       " 777,\n",
       " 586,\n",
       " 256,\n",
       " 602,\n",
       " 596,\n",
       " 271,\n",
       " 249,\n",
       " 144,\n",
       " 223,\n",
       " 849,\n",
       " 247,\n",
       " 383,\n",
       " 279,\n",
       " 164,\n",
       " 346,\n",
       " 433,\n",
       " 659,\n",
       " 665,\n",
       " 202,\n",
       " 308,\n",
       " 119,\n",
       " 305,\n",
       " 342,\n",
       " 423,\n",
       " 531,\n",
       " 123,\n",
       " 349,\n",
       " 180,\n",
       " 117,\n",
       " 596,\n",
       " 759,\n",
       " 343,\n",
       " 817,\n",
       " 671,\n",
       " 223,\n",
       " 401,\n",
       " 264,\n",
       " 152,\n",
       " 477,\n",
       " 645,\n",
       " 466,\n",
       " 243,\n",
       " 575,\n",
       " 470,\n",
       " 111,\n",
       " 813,\n",
       " 860,\n",
       " 236,\n",
       " 501,\n",
       " 446,\n",
       " 478,\n",
       " 411,\n",
       " 369,\n",
       " 923,\n",
       " 175,\n",
       " 812,\n",
       " 383,\n",
       " 328,\n",
       " 416,\n",
       " 756,\n",
       " 435,\n",
       " 128,\n",
       " 436,\n",
       " 651,\n",
       " 551,\n",
       " 451,\n",
       " 192,\n",
       " 792,\n",
       " 163,\n",
       " 941,\n",
       " 362,\n",
       " 221,\n",
       " 144,\n",
       " 224,\n",
       " 222,\n",
       " 320,\n",
       " 766,\n",
       " 495,\n",
       " 323,\n",
       " 602,\n",
       " 644,\n",
       " 755,\n",
       " 413,\n",
       " 108,\n",
       " 809,\n",
       " 172,\n",
       " 143,\n",
       " 706,\n",
       " 129,\n",
       " 604,\n",
       " 186,\n",
       " 458,\n",
       " 125,\n",
       " 321,\n",
       " 213,\n",
       " 281,\n",
       " 552,\n",
       " 393,\n",
       " 276,\n",
       " 866,\n",
       " 160,\n",
       " 459,\n",
       " 136,\n",
       " 112,\n",
       " 174,\n",
       " 448,\n",
       " 378,\n",
       " 271,\n",
       " 741,\n",
       " 158,\n",
       " 734,\n",
       " 458,\n",
       " 135,\n",
       " 572,\n",
       " 682,\n",
       " 420,\n",
       " 151,\n",
       " 808,\n",
       " 158,\n",
       " 153,\n",
       " 442,\n",
       " 215,\n",
       " 144,\n",
       " 797,\n",
       " 661,\n",
       " 308,\n",
       " 155,\n",
       " 627,\n",
       " 210,\n",
       " 675,\n",
       " 870,\n",
       " 398,\n",
       " 566,\n",
       " 875,\n",
       " 396,\n",
       " 187,\n",
       " 589,\n",
       " 510,\n",
       " 831,\n",
       " 275,\n",
       " 118,\n",
       " 510,\n",
       " 597,\n",
       " 385,\n",
       " 151,\n",
       " 145,\n",
       " 743,\n",
       " 224,\n",
       " 168,\n",
       " 108,\n",
       " 152,\n",
       " 900,\n",
       " 134,\n",
       " 404,\n",
       " 158,\n",
       " 678,\n",
       " 542,\n",
       " 567,\n",
       " 261,\n",
       " 523,\n",
       " 301,\n",
       " 641,\n",
       " 163,\n",
       " 160,\n",
       " 790,\n",
       " 100,\n",
       " 671,\n",
       " 502,\n",
       " 863,\n",
       " 321,\n",
       " 499,\n",
       " 477,\n",
       " 422,\n",
       " 427,\n",
       " 232,\n",
       " 729,\n",
       " 742,\n",
       " 402,\n",
       " 636,\n",
       " 768,\n",
       " 572,\n",
       " 335,\n",
       " 466,\n",
       " 600,\n",
       " 434,\n",
       " 333,\n",
       " 147,\n",
       " 262,\n",
       " 187,\n",
       " 714,\n",
       " 582,\n",
       " 123,\n",
       " 735,\n",
       " 633,\n",
       " 564,\n",
       " 369,\n",
       " 482,\n",
       " 225,\n",
       " 149,\n",
       " 633,\n",
       " 494,\n",
       " 388,\n",
       " 278,\n",
       " 786,\n",
       " 120,\n",
       " 643,\n",
       " 646,\n",
       " 450,\n",
       " 304,\n",
       " 159,\n",
       " 336,\n",
       " 662,\n",
       " 768,\n",
       " 177,\n",
       " 574,\n",
       " 475,\n",
       " 208,\n",
       " 287,\n",
       " 676,\n",
       " ...]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length_array = [len(seq) for seq in X_train.astype(str)]\n",
    "length_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7a3986d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 5688., 60116., 51155., 43608., 35785., 28959., 22513., 17882.,\n",
       "        14120.,  8174.]),\n",
       " array([ 12., 110., 208., 306., 404., 502., 600., 698., 796., 894., 992.]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(length_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "73266915",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_token = [text_to_word_sequence(_) for _ in X_train.astype(str)]\n",
    "X_val_token = [text_to_word_sequence(_) for _ in X_val.astype(str)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "efcd329c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learn embedding representation of words in reviews\n",
    "word2vec = Word2Vec(sentences=X_train_token, vector_size=50, min_count=5) #Reduced vector size from 100 to 50\n",
    "# Store words and trained embeddings in wv\n",
    "wv = word2vec.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2c462448",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hardly', 0.6065987348556519),\n",
       " ('neither', 0.5428374409675598),\n",
       " ('barely', 0.5366898775100708),\n",
       " ('never', 0.4965769350528717),\n",
       " ('yes', 0.4915393888950348),\n",
       " ('trick', 0.4873194992542267),\n",
       " ('pretty', 0.4866960048675537),\n",
       " ('though', 0.4774913489818573),\n",
       " ('nothing', 0.47464799880981445),\n",
       " ('otherwise', 0.465266615152359)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar(\"not\", topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fbd8b6c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52374"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wv.key_to_index.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657f7bb0",
   "metadata": {},
   "source": [
    "# Optimized Embedding Function v2 - on disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af0fefc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "166f1bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sentence_batch_to_disk(wv_vectors, wv_vocab, vector_size, sentences_batch, batch_index, output_dir):\n",
    "    \"\"\"\n",
    "    Processes a sentence batch and saves the result to disk using pickle.\n",
    "    \"\"\"\n",
    "    batch_embeddings = []\n",
    "\n",
    "    for sentence in sentences_batch:\n",
    "        valid_words = [word for word in sentence if word in wv_vocab]\n",
    "        if valid_words:\n",
    "            embeddings = np.array([wv_vectors[word] for word in valid_words])\n",
    "        else:\n",
    "            embeddings = np.array([]).reshape(0, vector_size)\n",
    "\n",
    "        batch_embeddings.append(embeddings)\n",
    "\n",
    "    padded_embedding = pad_sequences(batch_embeddings, maxlen=400, padding='post', value=0, dtype='float32')\n",
    "\n",
    "    # Save batch to disk\n",
    "    file_path = os.path.join(output_dir, f'batch_{batch_index}.pkl')\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(padded_embedding, f)\n",
    "\n",
    "    return file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "122f221a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_optimized_to_disk(wv, sentences, batch_size=10, n_jobs=-1, output_dir='embeddings_batches'):\n",
    "    \"\"\"\n",
    "    Optimized embedding function that saves intermediate results to disk to avoid memory overload.\n",
    "    \"\"\"\n",
    "    print(f\"Processing {len(sentences)} sentences...\")\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    vocab = set(wv.key_to_index.keys())\n",
    "    vector_size = wv.vector_size\n",
    "    batches = [sentences[i:i + batch_size] for i in range(0, len(sentences), batch_size)]\n",
    "\n",
    "    print(f\"Created {len(batches)} batches of ~{batch_size} sentences each\")\n",
    "    print(f\"Using {n_jobs} parallel processes...\")\n",
    "\n",
    "    # Process and store results to disk in parallel\n",
    "    result_files = Parallel(n_jobs=n_jobs, verbose=1, backend='loky')(\n",
    "        delayed(process_sentence_batch_to_disk)(wv, vocab, vector_size, batch, idx, output_dir)\n",
    "        for idx, batch in enumerate(batches)\n",
    "    )\n",
    "\n",
    "    print(f\"Embeddings saved to disk at '{output_dir}'. Total: {len(result_files)} files.\")\n",
    "    return result_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "37426c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 50000 sentences...\n",
      "Created 1000 batches of ~50 sentences each\n",
      "Using -1 parallel processes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "2025-06-11 15:24:09.164825: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different 2025-06-11 15:24:09.164828: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-11 15:24:09.165028: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-11 15:24:09.165141: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-11 15:24:09.165149: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-11 15:24:09.165284: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-11 15:24:09.167958: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU wi2025-06-11 15:24:09.167958: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "ll not be used.\n",
      "2025-06-11 15:24:09.168039: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-11 15:24:09.168592: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-06-11 15:24:09.168683: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-06-11 15:24:09.169006: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different 2025-06-11 15:24:09.169097: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU winumerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "ll not be used.\n",
      "2025-06-11 15:24:09.169570: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-06-11 15:24:09.179796: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-11 15:24:09.180342: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-06-11 15:24:09.182445: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU wi2025-06-11 15:24:09.182448: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-06-11 15:24:09.182450: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-06-11 15:24:09.182499: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU wi2025-06-11 15:24:09.182499: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "ll not be used.\n",
      "2025-06-11 15:24:09.182535: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "ll not be used.\n",
      "2025-06-11 15:24:09.182675: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-06-11 15:24:09.183192: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-06-11 15:24:09.216062: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Att2025-06-11 15:24:09.216058: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "empting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-06-11 15:24:09.216270: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Att2025-06-11 15:24:09.216331: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "empting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-06-11 15:24:09.216705: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-06-11 15:24:09.217425: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-06-11 15:24:09.217808: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-06-11 15:24:09.218989: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-06-11 15:24:09.255677: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: A2025-06-11 15:24:09.255690: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-06-11 15:24:09.255728: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-06-11 15:24:09.257574: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-06-11 15:24:09.257613: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "ttempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-06-11 15:24:09.258604: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-06-11 15:24:09.258629: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-06-11 15:24:09.258684: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-06-11 15:24:09.258927: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-06-11 15:24:09.259370: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-06-11 15:24:09.262793: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-06-11 15:24:09.263503: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: 2025-06-11 15:24:09.263600: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-06-11 15:24:09.263770: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: A2025-06-11 15:24:09.263790: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "ttempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-06-11 15:24:09.263938: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-06-11 15:24:09.304034: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use avai2025-06-11 15:24:09.304087: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI AVX_VNNI_INT8 AVX_NE_CONVERT FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-06-11 15:24:09.304123: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use avai2025-06-11 15:24:09.304125: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI AVX_VNNI_INT8 AVX_NE_CONVERT FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "lable CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI AVX_VNNI_INT8 AVX_NE_CONVERT FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "lable CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI AVX_VNNI_INT8 AVX_NE_CONVERT FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-06-11 15:24:09.304285: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI AVX_VNNI_INT8 AVX_NE_CONVERT FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-06-11 15:24:09.304462: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI AVX_VNNI_INT8 AVX_NE_CONVERT FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-06-11 15:24:09.304943: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI AVX_VNNI_INT8 AVX_NE_CONVERT FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-06-11 15:24:09.308321: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI AVX_VNNI_INT8 AVX_NE_CONVERT FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-06-11 15:24:11.202312: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-06-11 15:24:11.203137: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-06-11 15:24:11.203453: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-06-11 15:24:11.203654: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-06-11 15:24:11.204073: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-06-11 15:24:11.204200: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-06-11 15:24:11.204570: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-06-11 15:24:11.214294: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    6.3s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   11.1s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:   19.6s\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:   28.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings saved to disk at 'train_embeddings_batches'. Total: 1000 files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:   32.4s finished\n"
     ]
    }
   ],
   "source": [
    "train_batch_size = 50 # 50 sentences to be processed as a batch for train\n",
    "\n",
    "train_len = 50000 # Limit of rows to process\n",
    "\n",
    "train_file_paths = embedding_optimized_to_disk(wv, X_train_token[:train_len], batch_size=train_batch_size, n_jobs=-1, output_dir='train_embeddings_batches')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f50c914b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 10000 sentences...\n",
      "Created 1000 batches of ~10 sentences each\n",
      "Using -1 parallel processes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  52 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=-1)]: Done 352 tasks      | elapsed:    6.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings saved to disk at 'val_embeddings_batches'. Total: 1000 files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:   10.0s finished\n"
     ]
    }
   ],
   "source": [
    "val_batch_size = int(train_batch_size * val_size) # percentage of\n",
    "\n",
    "val_len = int(train_len * val_size)\n",
    "\n",
    "val_file_paths = embedding_optimized_to_disk(wv, X_val_token[:val_len], batch_size=val_batch_size, n_jobs=-1, output_dir='val_embeddings_batches')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "84821259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python generator to load batches\n",
    "\n",
    "def batch_generator(pickle_dir, y_labels, num_files=20):\n",
    "    \"\"\"\n",
    "    Generator that yields (X_batch, y_batch) for model.fit.\n",
    "\n",
    "    pickle_dir: directory with pickled batches\n",
    "    y_labels: list or array with labels for all sentences (ordered)\n",
    "    batch_size: size of mini-batches for training\n",
    "\n",
    "    Assumes that y_labels are aligned with sentence order across batches.\n",
    "    \"\"\"\n",
    "    # List all batch files sorted by batch index\n",
    "    batch_files = sorted([f for f in os.listdir(pickle_dir) if f.endswith('.pkl')])\n",
    "\n",
    "    start_idx = 0  # track label indexing\n",
    "\n",
    "    while True:  # Loop forever for Keras generator\n",
    "        for batch_file in batch_files:\n",
    "            file_path = os.path.join(pickle_dir, batch_file)\n",
    "            with open(file_path, 'rb') as f:\n",
    "                batch_data = pickle.load(f)  # list of padded arrays for batch\n",
    "\n",
    "            batch_data = np.array(batch_data)  # convert list to array, shape: (batch_sentences, maxlen, vec_size)\n",
    "\n",
    "            # Extract corresponding labels for this batch\n",
    "            batch_size_sentences = batch_data.shape[0]\n",
    "            batch_labels = y_labels[start_idx:start_idx + batch_size_sentences]\n",
    "            start_idx += batch_size_sentences\n",
    "\n",
    "            # Yield mini-batches from this loaded batch\n",
    "            for i in range(0, batch_size_sentences, num_files):\n",
    "                X_batch = batch_data[i:i+num_files]\n",
    "                y_batch = batch_labels[i:i+num_files]\n",
    "                yield X_batch, y_batch\n",
    "\n",
    "        # Reset start_idx and repeat if you want infinite generator (for multiple epochs)\n",
    "        start_idx = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f3c629e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: <class 'numpy.ndarray'>\n",
      "Number of sentences: 50\n",
      "Shape of first sentence: (400, 50)\n",
      "First sentence array:\n",
      "[[ 4.771143   -1.733841    1.022136   ... -3.562158   -4.537744\n",
      "   0.5085196 ]\n",
      " [-1.0134104  -2.931391    0.3957037  ...  2.7209735  -0.30498168\n",
      "  -0.776015  ]\n",
      " [-0.98702693 -0.31360108 -0.18210432 ... -2.3908615  -2.3263366\n",
      "  -3.27819   ]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Path to one of your batch files\n",
    "file_path = \"/home/marcvicente/code/marcvice9/sentiscope/notebooks/train_embeddings_batches/batch_0.pkl\"\n",
    "\n",
    "# Load the pickled data\n",
    "with open(file_path, \"rb\") as f:\n",
    "    batch_data = pickle.load(f)\n",
    "\n",
    "# Inspect\n",
    "print(f\"Type: {type(batch_data)}\")                     # Usually list or np.ndarray\n",
    "print(f\"Number of sentences: {len(batch_data)}\")       # Should match batch size\n",
    "print(f\"Shape of first sentence: {batch_data[0].shape}\")  # Should be (<=maxlen, vector_dim)\n",
    "print(f\"First sentence array:\\n{batch_data[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "814960e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m X_train_gen, y_train_gen \u001b[38;5;241m=\u001b[39m batch_generator(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/marcvicente/code/marcvice9/sentiscope/notebooks/train_embeddings_batches\u001b[39m\u001b[38;5;124m\"\u001b[39m, y_train, num_files\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      2\u001b[0m X_train_gen\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "X_train_gen, y_train_gen = batch_generator(\"/home/marcvicente/code/marcvice9/sentiscope/notebooks/train_embeddings_batches\", y_train, num_files=1)\n",
    "X_train_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "92613bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def init_model():\n",
    "    model = Sequential()\n",
    "    model.add(layers.Masking())\n",
    "    model.add(layers.LSTM(300, activation='tanh', return_sequences=True))\n",
    "    model.add(layers.LSTM(100, activation='tanh'))\n",
    "    model.add(layers.Dense(100, activation='sigmoid'))\n",
    "    model.add(layers.Dense(50, activation='sigmoid'))\n",
    "    model.add(layers.Dense(20, activation='sigmoid'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "model = init_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0b32f71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_files = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "60d0bae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "steps_per_epoch = math.ceil(train_len / (train_batch_size * num_files))\n",
    "validation_steps = math.ceil(val_size / (val_batch_size * num_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f14039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 413ms/step - accuracy: 0.5443 - loss: 0.6917 - val_accuracy: 0.2000 - val_loss: 0.6973\n",
      "Epoch 2/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 457ms/step - accuracy: 0.5666 - loss: 0.6905 - val_accuracy: 0.8000 - val_loss: 0.6875\n",
      "Epoch 3/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 484ms/step - accuracy: 0.5070 - loss: 0.6956 - val_accuracy: 0.8000 - val_loss: 0.6602\n",
      "Epoch 4/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 541ms/step - accuracy: 0.5115 - loss: 0.6954 - val_accuracy: 0.2000 - val_loss: 0.7236\n",
      "Epoch 5/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 604ms/step - accuracy: 0.4816 - loss: 0.6974 - val_accuracy: 0.8000 - val_loss: 0.6626\n",
      "Epoch 6/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 557ms/step - accuracy: 0.5420 - loss: 0.6913 - val_accuracy: 0.2000 - val_loss: 0.7270\n",
      "Epoch 7/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 541ms/step - accuracy: 0.4783 - loss: 0.6962 - val_accuracy: 0.8000 - val_loss: 0.6509\n",
      "Epoch 8/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 596ms/step - accuracy: 0.4654 - loss: 0.6962 - val_accuracy: 0.8000 - val_loss: 0.6781\n",
      "Epoch 9/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 532ms/step - accuracy: 0.4916 - loss: 0.6956 - val_accuracy: 0.2000 - val_loss: 0.7015\n",
      "Epoch 10/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 582ms/step - accuracy: 0.5384 - loss: 0.6947 - val_accuracy: 0.8000 - val_loss: 0.6752\n",
      "Epoch 11/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 545ms/step - accuracy: 0.5403 - loss: 0.6928 - val_accuracy: 0.2000 - val_loss: 0.7089\n",
      "Epoch 12/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 526ms/step - accuracy: 0.4636 - loss: 0.6966 - val_accuracy: 0.2000 - val_loss: 0.7001\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f47884d9240>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "\n",
    "model.fit(batch_generator(\"train_embeddings_batches\", y_train, num_files=num_files), # We are processing 20 train files with 50 sentences each\n",
    "          steps_per_epoch=steps_per_epoch,\n",
    "          epochs=100,\n",
    "          validation_data=batch_generator(\"val_embeddings_batches\", y_val, num_files=num_files),\n",
    "          validation_steps=validation_steps,\n",
    "          callbacks=[es]\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "984ee42f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ masking (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Masking</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">5,680</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">315</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ masking (\u001b[38;5;33mMasking\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m400\u001b[0m, \u001b[38;5;34m50\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)             │         \u001b[38;5;34m5,680\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m)             │           \u001b[38;5;34m315\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m16\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">12,024</span> (46.97 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m12,024\u001b[0m (46.97 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,011</span> (23.48 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m6,011\u001b[0m (23.48 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,013</span> (23.49 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m6,013\u001b[0m (23.49 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "80626ca6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 400, 50)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = os.path.join('train_embeddings_batches', 'batch_0.pkl')\n",
    "with open(file_path, 'rb') as f:\n",
    "    batch_data = pickle.load(f)  # list of padded arrays for batch\n",
    "\n",
    "batch_data = np.array(batch_data)  # convert list to array, shape: (batch_sentences, maxlen, vec_size)\n",
    "\n",
    "# Extract corresponding labels for this batch\n",
    "batch_size_sentences = batch_data.shape[0]\n",
    "\n",
    "text_pred = batch_data[0,:,:].reshape((1,400,50))\n",
    "text_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1ef01672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.48299447]], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(text_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "95c31ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = batch_generator(\"/home/marcvicente/code/marcvice9/sentiscope/notebooks/train_embeddings_batches\", y_train, num_files=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6c56fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'generator' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mgen\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'generator' object is not subscriptable"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5caf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "\n",
    "model.fit(batch_generator(\"/home/marcvicente/code/marcvice9/sentiscope/notebooks/train_embeddings_batches\", y_train[:int(train_batch_size*num_files)], num_files=num_files), # We are processing 20 train files with 50 sentences each\n",
    "          batch_size = 32,\n",
    "          epochs=100,\n",
    "          validation_data=batch_generator(\"/home/marcvicente/code/marcvice9/sentiscope/notebooks/val_embeddings_batches\", y_val[:int(val_batch_size*num_files)], num_files=num_files),\n",
    "          callbacks=[es]\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d0fe10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.48374757,  0.6616675 , -1.1716766 , ...,  0.8581864 ,\n",
       "        -0.11804877,  4.282535  ],\n",
       "       [-2.1587808 ,  3.0193887 , -0.34005904, ...,  0.07387018,\n",
       "        -0.45454714,  0.9833273 ],\n",
       "       [ 0.05201125,  2.1590605 , -0.36464986, ..., -0.04271401,\n",
       "         2.595512  ,  2.5595753 ],\n",
       "       ...,\n",
       "       [-3.4569967 ,  0.24943529,  1.5466684 , ..., -3.0612948 ,\n",
       "         0.7080936 ,  0.7968398 ],\n",
       "       [ 1.2508168 ,  1.3676991 ,  1.4378947 , ...,  2.0060124 ,\n",
       "         0.11168565,  2.4529166 ],\n",
       "       [-3.2289999 , -2.9927754 , -4.299237  , ...,  0.94018126,\n",
       "        -1.6745969 , -0.54463863]], dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_padded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fe5456",
   "metadata": {},
   "source": [
    "# Optimized Embedding Function v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cf20318c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1579a4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sentence_batch(wv_vectors, wv_vocab, vector_size, sentences_batch):\n",
    "    \"\"\"\n",
    "    Process a batch of sentences efficiently.\n",
    "\n",
    "    Args:\n",
    "        wv_vectors: Word2Vec vectors array\n",
    "        wv_vocab: Set of vocabulary words (for O(1) lookup)\n",
    "        vector_size: Dimension of word vectors\n",
    "        sentences_batch: List of sentences to process\n",
    "\n",
    "    Returns:\n",
    "        List of numpy arrays (embeddings for each sentence)\n",
    "    \"\"\"\n",
    "    batch_embeddings = []\n",
    "\n",
    "    for sentence in sentences_batch:\n",
    "        # Filter valid words using set lookup (O(1))\n",
    "        valid_words = [word for word in sentence if word in wv_vocab]\n",
    "\n",
    "        if valid_words:\n",
    "            # Get embeddings for valid words\n",
    "            embeddings = np.array([wv_vectors[word] for word in valid_words])\n",
    "        else:\n",
    "            # Empty sentence or no valid words\n",
    "            embeddings = np.array([]).reshape(0, vector_size)\n",
    "\n",
    "        batch_embeddings.append(embeddings)\n",
    "\n",
    "    return batch_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12cc68dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_optimized(wv, sentences, batch_size=10, n_jobs=-1):\n",
    "    \"\"\"\n",
    "    Optimized embedding function using joblib parallelization.\n",
    "\n",
    "    Args:\n",
    "        wv: Word2Vec vectors object\n",
    "        sentences: List of tokenized sentences\n",
    "        batch_size: Number of sentences per batch\n",
    "        n_jobs: Number of parallel jobs (-1 for all cores)\n",
    "\n",
    "    Returns:\n",
    "        List of numpy arrays (embeddings for each sentence)\n",
    "    \"\"\"\n",
    "    print(f\"Processing {len(sentences)} sentences...\")\n",
    "\n",
    "    # Pre-compute vocabulary set for O(1) lookups\n",
    "    vocab = set(wv.key_to_index.keys())\n",
    "    vector_size = wv.vector_size\n",
    "\n",
    "    # Split sentences into batches\n",
    "    batches = [sentences[i:i + batch_size]\n",
    "               for i in range(0, len(sentences), batch_size)]\n",
    "\n",
    "    print(f\"Created {len(batches)} batches of ~{batch_size} sentences each\")\n",
    "    print(f\"Using {n_jobs} parallel processes...\")\n",
    "\n",
    "    # Process batches in parallel\n",
    "    results = Parallel(n_jobs=n_jobs, verbose=1, backend='loky')(\n",
    "        delayed(process_sentence_batch)(wv, vocab, vector_size, batch)\n",
    "        for batch in batches\n",
    "    )\n",
    "\n",
    "    # Flatten results\n",
    "    all_embeddings = []\n",
    "    for batch_result in results:\n",
    "        all_embeddings.extend(batch_result)\n",
    "\n",
    "    print(f\"Successfully processed {len(all_embeddings)} sentences\")\n",
    "    return all_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a22fde94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 288000 sentences...\n",
      "Created 28800 batches of ~10 sentences each\n",
      "Using -1 parallel processes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    3.2s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:    9.6s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:   20.2s\n",
      "[Parallel(n_jobs=-1)]: Done 1088 tasks      | elapsed:   32.8s\n",
      "[Parallel(n_jobs=-1)]: Done 2008 tasks      | elapsed:   49.8s\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "X_train_embedded = embedding_optimized(wv, X_train_token, batch_size=10, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f257c922",
   "metadata": {},
   "source": [
    "# Not-Optimized Embedding Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c21e66ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88769288",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_sentence(word2vec, sentence):\n",
    "    wv = word2vec.wv\n",
    "    res_matrix = []\n",
    "\n",
    "    for word in sentence:\n",
    "\n",
    "        if word in wv:\n",
    "            res_matrix.append(wv[word])\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    return np.array(res_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "993ee87d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "106"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train_token[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "86a8ccde",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checks\n",
    "embedded_sentence = embed_sentence(word2vec, X_train_token[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "19e20516",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(embedded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e94a8dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(type(embedded_sentence) == np.ndarray)\n",
    "#assert(embedded_sentence.shape == (120, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894166e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def embedding(word2vec, sentences):\n",
    "\n",
    "    sentences_matrix = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentences_matrix.append(embed_sentence(word2vec, sentence))\n",
    "\n",
    "    return sentences_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eee104a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_embedded = embedding(word2vec, X_train_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e91d3879",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train_embedded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3680129e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train_token[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac735d56",
   "metadata": {},
   "source": [
    "# Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f4ffd5",
   "metadata": {},
   "source": [
    "from tensorflow.keras.utils import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3b872a20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train_embedded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e3cc49d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_padded_batches(X_embedded, batch_size, maxlen, save_path):\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    for i in range(0, len(X_embedded), batch_size):\n",
    "        batch = X_embedded[i:i+batch_size]\n",
    "        padded = pad_sequences(batch, maxlen=maxlen, padding='post', dtype='float32')\n",
    "        np.save(os.path.join(save_path, f\"batch_{i//batch_size}.npy\"), padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cd68511d",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_padded_batches(X_train_embedded, batch_size=100, maxlen=400, save_path=\"padded_embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aa494c6c",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 21.5 GiB for an array with shape (288000, 400, 50) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m X_train_pad_s \u001b[38;5;241m=\u001b[39m \u001b[43mpad_sequences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_embedded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfloat32\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxlen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m400\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/sentiscope/lib/python3.10/site-packages/keras/src/utils/sequence_utils.py:113\u001b[0m, in \u001b[0;36mpad_sequences\u001b[0;34m(sequences, maxlen, dtype, padding, truncating, value)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dtype_str:\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    108\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`dtype` \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not compatible with `value`\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms type: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    109\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mYou should set `dtype=object` for variable length \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    110\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    111\u001b[0m     )\n\u001b[0;32m--> 113\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxlen\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msample_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(sequences):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[0;32m~/.pyenv/versions/sentiscope/lib/python3.10/site-packages/numpy/core/numeric.py:329\u001b[0m, in \u001b[0;36mfull\u001b[0;34m(shape, fill_value, dtype, order, like)\u001b[0m\n\u001b[1;32m    327\u001b[0m     fill_value \u001b[38;5;241m=\u001b[39m asarray(fill_value)\n\u001b[1;32m    328\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m fill_value\u001b[38;5;241m.\u001b[39mdtype\n\u001b[0;32m--> 329\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    330\u001b[0m multiarray\u001b[38;5;241m.\u001b[39mcopyto(a, fill_value, casting\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munsafe\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m a\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 21.5 GiB for an array with shape (288000, 400, 50) and data type float32"
     ]
    }
   ],
   "source": [
    "X_train_pad_s = pad_sequences(X_train_embedded, dtype=\"float32\", padding='post', value=0, maxlen=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27eac522",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_pad_s = pad_sequences(X_val_embedded, dtype=\"float32\", padding='post', value=0, maxlen=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentiscope",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
